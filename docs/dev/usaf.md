This is a document describing the situation with USAF customer and the workflow as of now (31/08/2020)

** Please update this document as soon as it get irrelevant**

## Abstract

USAF is USA Army Forces and they have very strict requirements to security. 
This means that they cannot just take our public images and use them. They require the images to be based on
hardened (super secure) base images and to inspect any scripts, downloaded artifacts etc.

That's why they require separate dockerfiles which we (MongoDB) needs to prepare for them.

## History
Historically we had a dockerfile generation process when the Dockerfiles for USAF were generated by us and we updated 
them/uploaded during the release process so they appeared in the [external repository](https://repo1.dsop.io/dsop/mongodb/mongodb-enterprise) and 
could be used there (Rodrigo is the main expert in this)

## Now
Now it looks like this automation is not used anymore and the dockerfiles are being prepared manually.
As of now the main goal is to pass all checks/scans and get approval for the dockerfiles.
Later this process may be improved and automated.

### Workflow
The [repository](https://repo1.dsop.io/dsop/mongodb/mongodb-enterprise) contains 6 projects - one per each image.
Before commiting to the repository it's necessary go get registered using SSO.
After this the GIT token needs to be generated and it must be used during push operations as the password!
Any changes are done through Merge requests - they must be assigned to Matt Vasquez.
The general requirements to each vendor: https://repo1.dsop.io/dsop/dccscr/-/tree/master/contributor-onboarding
(very important to read before doing any work!)

### Some guidelines
* all docker arguments (`ARG`) must have the default - otherwise the building tools won't work correctly
* the general principle: it's ok to use upstream images or external tar files in the image if they meet the following requirements:
** the file must be included into `downloads.json` document with its SHA. It then can be referenced form the dockerfile - it 
will be downloaded into the local repo by the building tools
** it's not allowed to copy shell scripts from the upstream images though - they must be explicitly added to `/scripts` 
directory 

### Some important emails:

#### 1
Vendors do not have access to the Jenkins pipeline. If there is a issue with the pipeline please work through me and I can assist.

This container “mongodb-ops-manager-appdb-init" has already been pushed, merged into development and had security scan justifications completed/submitted. Looks like Anton L made a separate topic branch and changed the official image to use a empty VERSION argument for its version tag opposed to a statically set version number, which is causing issues as pipeline because it cannot pass arguments to an empty argument at build time. Changes were also made to the scripts directory and instead copied shell scripts from the official image instead. We have a firm requirement that any scripts used within the Dockerfile must be placed in the scripts directory in the project and copied in.

This is explained in the container hardening guide here:
https://repo1.dsop.io/dsop/dccscr/-/tree/master/contributor-onboarding

I would suggest leaving this one alone for now since you will have justify the scans again and have our security team re-review the justifications, which is currently being done for the first submission. Only very minor changes such as removing the mkdir statements and copying the license from the official image instead of the scripts directory are applicable here. This can be done in the next version release.

I see a push made for the mongodb-ops-manager-init MR. This will need the VERSION argument set to a version number and keep ALL the shell scripts in the scripts directory in the project to be copied into the container via the COPY statement. Once this has been corrected I can approve the MR and provide the scan for justifications.

#### 2

The easiest and most preferred way is to copy binaries is to copy from the upstream image in a multi-stage build (like the Ops Manager App DB Init Container was done with the built Golang binaries). The other way which we allow is to use the download.yaml/json file which is what we use in our pipeline to download external content locally, including the docker image to be used in the multistage build. Theses are the two main methods which are allowed. This is mainly for DoD security purposes and this process has been approved.

 

Depending on how many binaries need to be copied, should dictate which method is best. I would recommend for one or a few binaries, use the multistage build copy method and for many binaries use the external tarball method via download.yaml/json if possible. Either way is acceptable.

The scripts folder should only be used for scripts.

Here is an example of the download.yaml format:
https://repo1.dsop.io/dsop/dccscr/-/blob/master/contributor-onboarding/download.yaml

You can also refer to the download.json of the Ops Manager App DB Init Container on how to download the docker image to be imported:
https://repo1.dsop.io/dsop/mongodb/mongodb-enterprise/mongodb-ops-manager-appdb-init/-/blob/development/download.json

Just make sure the tag you specify in the download.yaml/json for the docker image matches exactly in the Dockerfile or it will fail in the pipeline. We support both json and yaml formats for the Download file.

Basically the download.yaml/json file is used to import content (binaries, libs, rpms, deps, configs, etc) needed for the docker build into our air-gapped build environment. Files such as tarballs get saved in the same directory as the Dockerfile, so you can use a regular COPY statement in the Dockerfile as you would if the tarball were locally in the projects root directory. For docker images used in a multistage build, our pipeline will download and then import in to a local docker registry to be used in the build stage. So the image that is being pulled in the Dockerfile is not coming from docker hub directly but from our internal docker registry as the build stage is air-gapped as mentioned earlier. So keep in mind that any wget or curl calls that download content within the Dockerfile will have to be taken out and converted to use the download.yaml/json and be copied in via a COPY statement.

Hopefully this all is starting to make sense. I know this process is proprietary and our documentation doesn’t cover all the details of the process we have in place. So please don’t hesitate to ask me questions or bring up concerns with the process. I am also available for a call or zoom session if need be.
