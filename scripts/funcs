#!/usr/bin/env bash

# Set of common functions used by different scripts (evergreen, dev, build)
# Important: all the locations in the functions are relative to the root of the project so the calling
# script must make sure to cd there

fatal() {
    error "$1"
    exit 1
}

error() {
    echo "(!!) $1"
    return
}

title() {
    echo "=> $1"
}

header() {
    echo
    echo "--------------------------------------------------"
    echo "$1"
    echo "--------------------------------------------------"
}
check_env_var() {
    var_name="$1"
    msg="$2"
    set +u
    if [[ -z "${!var_name}" ]]; then
        echo "${msg}"
        exit 1
    fi
}

check_app() {
    var="$1"
    msg="$2"
    if ! which "${var}" > /dev/null; then
        echo "${msg}"
        exit 1
    fi
}

check_mandatory_param() {
    param="${1-}"
    param_name="${2-}"
    if [[ -z "$param" ]]; then
        fatal "Parameter $param_name must be specified!"
    fi
}
ensure_namespace() {
    namespace="${1}"
    tmp_file=$(mktemp)
    cat <<EOF > "${tmp_file}"
apiVersion: v1
kind: Namespace
metadata:
  name: ${namespace}
  labels:
    evg: task
  annotations:
    evg/version: "https://evergreen.mongodb.com/version/${VERSION_ID:-'not-specified'}"
    evg/task-name: ${TASK_NAME:-'not-specified'}
    evg/task: "https://evergreen.mongodb.com/task/${TASK_ID:-'not-specified'}"
    evg/mms-version: "${MMS_VERSION:-'not-specified'}"
EOF

    if kubectl get "ns/${namespace}" -o name > /dev/null; then
        echo "Namespace already exists!"
    else
        echo "Creating new namespace: ${namespace}"
        cat "${tmp_file}"
        kubectl create -f "${tmp_file}"
    fi
}

wait_for_namespace_to_be_deleted(){
    namespace=${1}
    while kubectl get ns | grep -q ${namespace} ; do printf .; sleep 1; done;
}

# the function installs Ops Manager into Kubernetes cluster if it's not installed already
# Note, that this requires the k8s node with 4 Gb of free memory
ensure_ops_manager_k8s() {
    ops_manager_namespace="${1}"
    ops_manager_version="${2}"
    node_port="${3}"

    if [ -z $ops_manager_namespace ]; then
        error "ops_manager_namespace must be set!"
        exit 1
    fi
    if [ -z $ops_manager_version ]; then
        error "ops_manager_version must be set!"
        exit 1
    fi
    if [ -z $node_port ]; then
        error "node_port must be set!"
        exit 1
    fi

    echo "##### Installing Ops Manager ${ops_manager_version} (namespace ${ops_manager_namespace} into Kubernetes..."

    if is_ops_manager_down ${ops_manager_namespace}; then
        echo "Ops Manager is not installed in this cluster. Doing it now."
        echo "Run \"make -C docker/mongodb-enterprise-ops-manager-dev IMAGE_VERSION=${ops_manager_version} push\" to update Ops Manager image in ECR if necessary"

        # Building Ops Manager image if we want to run it in local Kubernetes cluster
        if [[ ${REPO_TYPE:-'ecr'} = "local" ]]; then
            make -C docker/mongodb-enterprise-ops-manager-dev IMAGE_VERSION=${ops_manager_version} build
        fi

        echo "Installing Ops Manager ${ops_manager_version}"

        helm template -f scripts/evergreen/deployments/values-ops-manager.yaml \
            --set namespace=${ops_manager_namespace} \
            --set opsManager.version=${ops_manager_version} \
            --set opsManager.host="ops-manager.${ops_manager_namespace}.svc.cluster.local" \
            --set opsManager.nodePort=${node_port} \
            docker/mongodb-enterprise-ops-manager-dev/helm_chart > mongodb-enterprise-ops-manager.yaml

        echo

        kubectl delete -f mongodb-enterprise-ops-manager.yaml || true
        kubectl apply -f mongodb-enterprise-ops-manager.yaml

        rm mongodb-enterprise-ops-manager.yaml

        echo "Waiting until Statefulset for Ops Manager is running..."

        timeout "1m" bash -c \
          'while [[ $(kubectl -n '${ops_manager_namespace}' get statefulset mongodb-enterprise-ops-manager -o jsonpath="{.status.currentReplicas}") != "1" ]]; do sleep 1; done'

        echo "Statefulset is ready, waiting for Ops Manager to start..."

        # We can't communicate with Ops Manager if it is inside Kubernetes, so we just
        # wait for this command to succeed.
        # Note, that for OM check for "status.phase==Running" may not work - we need to check the container status
        timeout "10m" bash -c \
          "while ! kubectl get pod/mongodb-enterprise-ops-manager-0 -o jsonpath=\"{.status.containerStatuses[0].ready}\" -n ${ops_manager_namespace} | grep -q \"true\"; do sleep 4; done" || true

        if ! kubectl get pod/mongodb-enterprise-ops-manager-0 -o jsonpath="{.status.containerStatuses[0].ready}" -n ${ops_manager_namespace} | grep -q "true"; then
            error "Ops Manager hasn't started!"
            kubectl describe pod/mongodb-enterprise-ops-manager-0 -n "${ops_manager_namespace}"
            exit 1
        fi
        echo "Pod is ready, waiting for the admin to get registered..."

        timeout "4m" bash -c \
           "while ! kubectl -n ${ops_manager_namespace} exec mongodb-enterprise-ops-manager-0 ls \"/opt/mongodb/mms/env/.ops-manager-env\" &>/dev/null; do sleep 2; done" || true

        if ! kubectl -n "${ops_manager_namespace}" exec mongodb-enterprise-ops-manager-0 ls "/opt/mongodb/mms/env/.ops-manager-env" &>/dev/null; then
            error "No env file found in Ops Manager!"
            # todo print logs when OM image is fixed and rebuilt
            # kubectl logs  -n operator-testing mongodb-enterprise-ops-manager-0

            exit 1
        fi

        # we try to open ports for both security groups - in kops and openshift clusters
        ensure_vpc_rules "us-east-1" "openshift-test-workersecgroup"
        ensure_vpc_rules "us-east-2" "nodes.e2e"

        echo "Ops Manager is installed in this cluster. A new user will be added for automated tests to run."
    else
        echo "Ops Manager is already installed in this cluster."
        echo "If you want to start with a fresh Ops Manager installation, please delete the \"${ops_manager_namespace}\" namespace."
    fi

    echo "##### Ops Manager is ready"

    print_om_endpoint "" ${ops_manager_namespace} ${node_port}
}

print_om_endpoint() {
    ops_manager_namespace="${2}"
    if [ -z "$ops_manager_namespace" ]; then
        error "No ops_manager_namespace set: not able to find Ops Manager external IP"
    else
        node_port="${3}"
        external_ip="$(kubectl get nodes -o wide | grep $(kubectl get pods/mongodb-enterprise-ops-manager-0 -n ${ops_manager_namespace} -o wide | tail -n 1 | awk '{print $7}') | awk '{print $7}')"

        echo "Use the following address to access Ops Manager from the browser: http://${external_ip}:${node_port}"
    fi
}

print_perpetual_om_endpoint() {
    echo "Use the following address to access Ops Manager from the browser: ${OM_BASE_URL}"
}

ensure_vpc_rules() {
    region="$1"
    prefix="$2"
    group_id=$(aws ec2 describe-security-groups --region "${region}" | jq -r '.SecurityGroups[] | select(.GroupName | startswith( "'"$prefix"'")) | .GroupId')

    if [[ -z $group_id ]]; then
        echo "Failed to find group id for EC2 security group with prefix $prefix"
        return 1
    fi
    # note, that the attempt to add the same rule will result in "already exists" error - so we just ignore errors by now
    # open multiple node ports for different versions of Ops Manager
    for i in $(seq 30039 30043); do
        aws ec2 authorize-security-group-ingress --region "${region}" --group-id ${group_id} --protocol tcp --port ${i} --cidr "0.0.0.0/0" 2>/dev/null || true
        echo "Opened port ${i} in AWS for the security group with prefix $prefix (group id: $group_id)"
    done

}
# returns true if the namespace doesn't exist or if the OM pod is not running and this lasts for more than or equal to 1 hour.
# This is supposed to work for concurrent builds when one of the builds is starting Ops Manager (ideally we need to check for 10 minutes
# but parsing dates in bash is hard :( )
is_ops_manager_down() {
    ! kubectl get namespace/${1} &> /dev/null || \
    ! kubectl get pod/mongodb-enterprise-ops-manager-0 -n ${1} 2>/dev/null ||
    kubectl get pod/mongodb-enterprise-ops-manager-0 -o jsonpath="{.status.containerStatuses[0].ready}" -n ${1} | grep -q "false"

}
# the function installs Ops Manager into Evergreen host via ego
ensure_ops_manager_evg() {
    om_package="$1"

    title "Installing Ops Manager into Evergreen using ego (from $om_package)"

    check_app "evg" \
                "evg script is not installed or you need to switch virtual environment, follow instruction at https://wiki.corp.mongodb.com/display/MMS/Ops+Manager+Release+setup+guide#OpsManagerReleasesetupguide-First-timeonly to make it work";

    # TODO this check can definitely be improved (walk through ubuntu1804-build hosts and find installed OMs)
    if evg list | grep "ubuntu1804-build" | grep -qi "running"; then
        OPS_MANAGER_HOST=$(evg list | grep "ubuntu1804-build" | grep -i "running" | head -n 1 | awk ' {print $3} ' | cut -d "@" -f 2)
        export OPS_MANAGER_HOST
        echo "There is an Evergreen instance (${OPS_MANAGER_HOST}) running already - skipping ego installaton"
        return
    fi

    # download ego script
    check_env_var "GITHUB_TOKEN" "To download ego you need to specify the 'GITHUB_TOKEN' environment variable (https://github.com/settings/tokens/new) with 'repo' scope"

    echo "Downloading ego script"
    curl -L "https://${GITHUB_TOKEN}@raw.githubusercontent.com/10gen/mms/master/scripts/ops_manager/ego" -o ego && chmod +x ego

    # provision evergreen host
    echo "Spawning new VM in Evergreen..."

    evg spawn ubuntu1804-build

    while ! evg list | grep "ubuntu1804-build"  | grep -qi "running"; do printf .; sleep 3; done

    host=$(evg list | awk ' {print $3} ' | cut -d "@" -f 2)

    ./ego seed "ubuntu@${host}"

    echo "Deploying Ops Manager from $om_package"

    # todo move version to some configuration
    # note, that we use OM 4.1 for development to differ from e2e suites (they use 4.0+)
    ./ego nohup "ubuntu@${host}" ego scenario_install_package_from_link --url "${om_package}" --central-url "http://${host}:9080"

    if ! ./ego tail "ubuntu@$host"; then
        # seems 'tail' throws an error after finish which is not relevant
        # another thing that is used in mms: './ego wait_for_open_port $host 9080'
        echo "WARN: Ego tail exited with non-zero code $?, but most of all everything is ok!"
    fi

    export OPS_MANAGER_HOST=${host}

    # add ssh host key to known hosts
    ssh-keyscan "$OPS_MANAGER_HOST" >> ~/.ssh/known_hosts

    rm ego

    title "Ops Manager is ready! Hostname: ${OPS_MANAGER_HOST}"
}

# Generates kube yaml configuration for the Operator from helm chart and deletes + installs the Operator to the Kubernetes cluster
redeploy_operator() {
    url="$1"
    version="${2:-latest}"
    ns="$3"
    watch_namespace="${4:-$ns}"
    pull_policy="${5:-Always}"
    managed_security_context="${6:-false}"
    timeout="${7:-1m}"

    echo "Installing the Operator (registry url: ${url}, version: ${version}, namespace: ${ns}, managed_security_context: ${managed_security_context}) to ${CLUSTER_NAME:-'e2e cluster'}..."

    check_app "helm" "helm is not installed, run 'make prerequisites' to install all necessary software"
    check_app "timeout" "coreutils is not installed, call \"brew install coreutils\""

    tmp_file=$(mktemp)
    helm template public/helm_chart \
         --set registry.repository="${url}" \
         --set registry.pullPolicy="${pull_policy}" \
         --set operator.env=dev \
         --set operator.version="${version}" \
         --set operator.watchNamespace="${watch_namespace}" \
         --set operator.name="${OPERATOR_NAME:=mongodb-enterprise-operator}" \
         --set database.name="${DATABASE_NAME:=mongodb-enterprise-database}" \
         --set opsManager.name="${OPS_MANAGER_NAME:=mongodb-enterprise-ops-manager}" \
         --set appDb.name="${APPDB_NAME:=mongodb-enterprise-appdb}" \
         --set namespace="${ns}" \
         --set managedSecurityContext="${managed_security_context}" \
         --set debug="${DEBUG-}" \
         --set debugPort="${DEBUG_PORT:-30042}" \
         > "${tmp_file}"

    if [[ "${OPERATOR_UPGRADE_IN_PROGRESS-}" != "stage2" ]]; then
        # note that we don't remove all objects from file as it contains CRDs as well (and they will be blocked if there
        # are existing mongodb resources)
        echo "Removing the Operator deployment"
        kubectl delete deployment ${OPERATOR_NAME} &> /dev/null || true
    fi

    echo "Deploying Operator"
    kubectl apply -f "${tmp_file}"
    rm "${tmp_file}"

    if [[ "${OPERATOR_UPGRADE_IN_PROGRESS-}" = "stage2" ]]; then
        echo "Doing Upgrade stage 2, need to wait for the new operator to start."
        echo "Forcing 30 seconds to make sure the *new* operator has started and we are not looking for the original one."
        sleep 30
    fi
    echo "Waiting until the Operator gets to Running state..."

    export OPERATOR_NAME
    timeout "${timeout}" bash -c \
        'while ! kubectl -n '"${ns}"' get pods -l app=${OPERATOR_NAME} -o jsonpath="{.items[0].status.phase}" 2>/dev/null | grep -q "Running" ; do printf .; sleep 1; done' || true

    # In the end let's check again and print the state
    if ! kubectl -n "${ns}" get pods -l "app=${OPERATOR_NAME}" -o jsonpath="{.items[0].status.phase}" | grep -q "Running"; then
        error "Operator hasn't reached RUNNING state after ${timeout}. The full yaml configuration for the pod is:"
        kubectl -n "${ns}" get pods -l "app=${OPERATOR_NAME}" -o yaml

        title "Operator failed to start, exiting"
        exit 1
    fi
    echo ""

    # if ! kubectl -n "${ns}" get crds | grep -q mongodb.mongodb.com ; then
    #     echo "mongodb.mongodb.com not found"
    #     kubectl get crd
    #     exit 1
    # fi

    title "The Operator successfully installed to the Kubernetes cluster"
}

# Checks minikube and starts if necessary
# (If any of "host", "kubelet" or "apiserver" are "Stopped" or "Error" or there is no "Running")
ensure_minikube() {
    if [[ ! $(minikube status) \
        || $(minikube status | grep -q "Stopped") \
        || $(minikube status | grep -q "Error") \
        || ! $(minikube status | grep "Running") ]]; then

        echo "Starting minikube (Kubernetes version 1.15.4) as it's not started..."
        # todo minikube parameters to configuration
        minikube start --kubernetes-version v1.15.4 --memory 5120
    fi
}

# Creates kops cluster and adds the support for dashboard
create_kops_cluster() {
    cluster_name="${1}"
    node_count=${2}
    node_volume_size=${3}
    node_size="${4}"
    master_size="${5}"
    zones="${6}"

    title "Creating kops cluster $cluster_name (master: $master_size, nodes: $master_size, zones: $zones)"

    check_env_var "KOPS_STATE_STORE" "Make sure you add \"export KOPS_STATE_STORE=s3://kube-om-state-store\" to your ~/.bashrc"

    echo "Make sure you use the latest version of kops (>= 1.14.0): 'brew upgrade kops'"
    kops create cluster \
         --node-count "$node_count" \
         --zones "${zones}" \
         --node-size "${node_size}" \
         --node-volume-size "${node_volume_size}" \
         --master-size="${master_size}" \
         --master-volume-size 16  \
         --kubernetes-version=v1.15.4 \
         --ssh-public-key=~/.ssh/id_rsa.pub \
         --authorization RBAC "${cluster_name}"

    kops create secret --name "${cluster_name}" sshpublickey admin -i ~/.ssh/id_rsa.pub

    kops update cluster "${cluster_name}" --yes

    echo "Waiting until kops cluster gets ready..."

    timeout "20m" bash -c 'while ! kops validate cluster $cluster_name &>/dev/null ; do printf .; sleep 5; done'

    title "Kops cluster $cluster_name is ready! Note, that you need to use 'admin' user if you want to ssh to the nodes"

    # see https://codeandunicorns.com/kubernetes-dashboard-kops/
    # also keep an eye on https://github.com/kubernetes/dashboard/releases - we should use 2.0.0 once it goes to GA
    title "Adding support for Kubernetes dashboard"
    kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml

    #To create a service account with access to default namespace
    kubectl create serviceaccount dashboard -n default

    #To create a cluster role bind. Connecting service account and cluster level access
    kubectl create clusterrolebinding dashboard-admin -n default \
     --clusterrole=cluster-admin \
     --serviceaccount=default:dashboard

    title "Kubernetes dashboard is installed, use \"make dashboard\" to open it"
}

# Returns a random string that can be used as a namespace.
generate_random_namespace() {
    random_namespace=$(LC_ALL=C tr -dc 'a-z0-9' </dev/urandom | head -c 20)
    doy=$(date +'%j')
    echo "a-${doy}-${random_namespace}z"
}

# gathers the information about K8s objects and writes it to the specified file handler (can be the stdout)
dump_diagnostic_information() {
    dump_file=$1

    if ! kubectl get mdbu -n "${PROJECT_NAMESPACE}" 2>&1 | grep -q "No resources found"; then
        header "MongoDBUser resources" >> "$dump_file"
        kubectl get mdbu -o yaml -n "${PROJECT_NAMESPACE}"  >> "$dump_file"
    fi

    if ! kubectl get mdb -n ${PROJECT_NAMESPACE} 2>&1 | grep -q "No resources found"; then
        header "Mongodb Resources" >> $dump_file
        kubectl get mdb -o yaml -n ${PROJECT_NAMESPACE}  >> $dump_file
    fi

    header "All namespace objects" >> "$dump_file"
    kubectl get all -n "${PROJECT_NAMESPACE}" >> "$dump_file"

    if ! kubectl get pvc -n "${PROJECT_NAMESPACE}" 2>&1 | grep -q "No resources found"; then
        header "Persistent Volume Claims" >> "$dump_file"
        kubectl get pvc -o yaml -n "${PROJECT_NAMESPACE}"  >> "$dump_file"
    fi

    # we don't output pods for the operator and tests (they will be output if they didn't manage to start)
    for pod in $(kubectl get pods -n "${PROJECT_NAMESPACE}" -o name | grep -v "${OPERATOR_NAME}"); do
        header "$pod" >> "$dump_file"
        # in general describe produces shorter output than "get -o yaml" and seems it is enough to diagnose the problem
        kubectl describe "${pod}" -n "${PROJECT_NAMESPACE}"  >> "$dump_file"
    done
}

ensure_ecr_repository() {
    local ecr_repo_name=$1
    if ! aws ecr describe-repositories | grep "${ecr_repo_name}" > /dev/null; then
        aws ecr create-repository --repository-name="${ecr_repo_name}"
    fi
}

om_download_url() {
    check_mandatory_param "${1-}" "om_version"
    om_version=${1}
    # first search in current releases
    link=$(curl -l https://info-mongodb-com.s3.amazonaws.com/com-download-center/ops_manager_release_archive.json | \
              jq -r '.currentReleases[] | select(.version == "'"${om_version}"'") | .platform[] | select(.package_format=="deb") | select(.arch=="x86_64") | .packages.links[] | select(.name=="tar.gz") | .download_link')

    # searching in old releases
    [[ -z ${link} ]] && link=$(curl -l https://info-mongodb-com.s3.amazonaws.com/com-download-center/ops_manager_release_archive.json | \
              jq -r '.oldReleases[] | select(.version == "'"${om_version}"'") | .platform[] | select(.package_format=="deb") | select(.arch=="x86_64") | .packages.links[] | select(.name=="tar.gz") | .download_link')

    echo ${link}
}
