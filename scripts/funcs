#!/usr/bin/env bash

# Set of common functions used by different scripts (evergreen, dev, build)
# Important: all the locations in the functions are relative to the root of the project so the calling
# script must make sure to cd there

fatal() {
    error "$1"
    exit 1
}

error() {
    echo "(!!) $1"
    return
}

title() {
    echo "=> $1"
}

header() {
    echo
    echo "--------------------------------------------------"
    echo "$1"
    echo "--------------------------------------------------"
}
check_env_var() {
    var_name="$1"
    msg="$2"
    set +u
    if [[ -z "${!var_name}" ]]; then
        echo "${msg}"
        exit 1
    fi
}

check_app() {
    var="$1"
    msg="$2"
    if ! which "${var}" > /dev/null; then
        echo "${msg}"
        exit 1
    fi
}

# wait_for_or_kill
# Waits for a given pid ${1} for a given amount of seconds ${2} to finish will
# return true (success) if the process finished before the timeout, false otherwise.
wait_for_or_kill() {
    pid=${1}
    timeout=${2}

    ( sleep "${timeout}" && kill -HUP "${pid}" ) 2> /dev/null & watcher=$!
    if wait "${pid}" 2> /dev/null; then
        pkill -HUP -P $watcher
        wait $watcher
        return 0
    fi

    return 1
}

ensure_namespace() {
    namespace="${1}"
    tmp_file=$(mktemp)
    cat <<EOF > "${tmp_file}"
apiVersion: v1
kind: Namespace
metadata:
  name: ${namespace}
  labels:
    evg/version-id: ${VERSION_ID}
    evg/task-name: ${TASK_NAME}
  annotations:
    evg/url: "https://evergreen.mongodb.com/version/${VERSION_ID}"
    evg/task-url: "https://evergreen.mongodb.com/task/${TASK_ID}"
EOF
    kubectl create -f "${tmp_file}"
}
# the function installs Ops Manager into Kubernetes cluster if it's not installed already
# Note, that this requires the k8s node with 4 Gb of free memory
ensure_ops_manager_k8s() {
    echo "##### Installing Ops Manager into Kubernetes..."

    if is_ops_manager_down; then
        echo "Ops Manager is not installed in this cluster. Doing it now."
        echo "Run \"make -C docker/mongodb-enterprise-ops-manager IMAGE_VERSION=<version> push\" to update Ops Manager image in ECR if necessary"
        echo "Also you'll need to update \"opsManager.version\" in scripts/evergreen/deployments/values-ops-manager.yaml"

        # Building Ops Manager image if we want to run it in local Kubernetes cluster
        if [[ ${REPO_TYPE:-'ecr'} = "local" ]]; then
            make -C docker/mongodb-enterprise-ops-manager build
        fi
        kubectl create namespace operator-testing  || true

        # Install Ops Manager
        helm template -f scripts/evergreen/deployments/values-ops-manager.yaml \
            docker/mongodb-enterprise-ops-manager/helm_chart > mongodb-enterprise-ops-manager.yaml

        kubectl delete -f mongodb-enterprise-ops-manager.yaml
        kubectl apply -f mongodb-enterprise-ops-manager.yaml

        rm mongodb-enterprise-ops-manager.yaml

        echo "Waiting until Statefulset for Ops Manager is running..."

        timeout "1m" bash -c \
          'while [[ $(kubectl -n operator-testing get statefulset mongodb-enterprise-ops-manager -o jsonpath="{.status.currentReplicas}") != "1" ]]; do sleep 1; done'

        echo "Statefulset is ready, waiting for the Ops Manager to start..."

        # We can't communicate with Ops Manager if it is inside Kubernetes, so we just
        # wait for this command to succeed.
        # Note, that for OM check for "status.phase==Running" may not work - we need to check the container status
        timeout "5m" bash -c \
          'while ! kubectl get pod/mongodb-enterprise-ops-manager-0 -o jsonpath="{.status.containerStatuses[0].ready}" -n operator-testing | grep -q "true"; do sleep 4; done' || true

        if ! kubectl get pod/mongodb-enterprise-ops-manager-0 -o jsonpath="{.status.containerStatuses[0].ready}" -n operator-testing | grep -q "true"; then
            error "Ops Manager hasn't started!"
            kubectl describe pod/mongodb-enterprise-ops-manager-0 -n operator-testing

            exit 1
        fi
        echo "Pod is ready, waiting for the admin to get registered..."

        # waiting until the om file is ready
        timeout "4m" bash -c \
           'while ! kubectl -n operator-testing exec mongodb-enterprise-ops-manager-0 ls "/opt/mongodb/mms/env/.ops-manager-env" &>/dev/null; do sleep 2; done' || true

        if ! kubectl -n operator-testing exec mongodb-enterprise-ops-manager-0 ls "/opt/mongodb/mms/env/.ops-manager-env" &>/dev/null; then
            error "No env file found in Ops Manager!"
            # todo print logs when OM image is fixed and rebuilt
            # kubectl logs  -n operator-testing mongodb-enterprise-ops-manager-0

            exit 1
        fi

        # we try to open ports for both security groups - in kops and openshift clusters
        ensure_vpc_rules "us-east-1" "openshift-test-workersecgroup"
        ensure_vpc_rules "us-east-2" "nodes.e2e"

        echo "Ops Manager is installed in this cluster. A new user will be added for automated tests to run."
    else
        echo "Ops Manager is already installed in this cluster."
        echo "If you want to start with a fresh Ops Manager installation, please delete the \"operator-testing\" namespace."
    fi

    echo "##### Ops Manager is ready"

    print_om_endpoint
}

print_om_endpoint() {
    ns="${1:-'<undefined>'}"
    external_ip="$(kubectl get nodes -o wide | grep $(kubectl get pods/mongodb-enterprise-ops-manager-0 -n operator-testing -o wide | tail -n 1 | awk '{print $7}') | awk '{print $7}')"

    echo "Use the following address to access Ops Manager from the browser: http://${external_ip}:30039 (namespace: ${ns})"
}

print_perpetual_om_endpoint() {
    ns="${1:-'<undefined>'}"
    echo "Use the following address to access Ops Manager from the browser: ${OM_BASE_URL} (namespace: ${ns})"
}

ensure_vpc_rules() {
    region="$1"
    prefix="$2"
    group_id=$(aws ec2 describe-security-groups --region "${region}" | jq -r '.SecurityGroups[] | select(.GroupName | startswith( "'"$prefix"'")) | .GroupId')

    if [[ -z $group_id ]]; then
        echo "Failed to find group id for EC2 security group with prefix $prefix"
        return 1
    fi
    # note, that the attempt to add the same rule will result in "already exists" error - so we just ignore errors by now
    aws ec2 authorize-security-group-ingress --region "${region}" --group-id ${group_id} --protocol tcp --port 30039 --cidr "0.0.0.0/0" 2>/dev/null || true

    echo "Opened port 30039 in AWS for the security group with prefix $prefix (group id: $group_id)"
}
# returns true if the namespace doesn't exist or if the OM pod is not running and this lasts for more than or equal to 1 hour.
# This is supposed to work for concurrent builds when one of the builds is starting Ops Manager (ideally we need to check for 10 minutes
# but parsing dates in bash is hard :( )
is_ops_manager_down() {
    # todo ideally we need to check for how long OM was down - if less than 5 minutes - then it may seem that it's being started
    # by another build
    # unfortunately "[[ $(kubectl get pod/mongodb-enterprise-ops-manager-0 -n operator-testing | tail -n 1 | awk '{ print $NF }' | sed 's/h//') -ge 1 ]])"
    # doesn't work here as time can be returned in hours and minutes ("2h" vs "57m")
 ! kubectl get namespace/operator-testing &> /dev/null || \
    kubectl get pod/mongodb-enterprise-ops-manager-0 -o jsonpath="{.status.containerStatuses[0].ready}" -n operator-testing | grep -q "false"

}
# the function installs Ops Manager into Evergreen host via ego
ensure_ops_manager_evg() {
    title "Installing Ops Manager into Evergreen using ego"

    check_app "evg" \
                "evg script is not installed or you need to switch virtual environment, follow instruction at https://wiki.corp.mongodb.com/display/MMS/Ops+Manager+Release+setup+guide#OpsManagerReleasesetupguide-First-timeonly to make it work";

    # TODO this check can definitely be improved (walk through ubuntu1804-build hosts and find installed OMs)
    if evg list | grep "ubuntu1804-build" | grep -qi "running"; then
        OPS_MANAGER_HOST=$(evg list | grep "ubuntu1804-build" | grep -i "running" | head -n 1 | awk ' {print $3} ' | cut -d "@" -f 2)
        export OPS_MANAGER_HOST
        echo "There is an Evergreen instance (${OPS_MANAGER_HOST}) running already - skipping ego installaton"
        return
    fi

    # download ego script
    check_env_var "GITHUB_TOKEN" "To download ego you need to specify the 'GITHUB_TOKEN' environment variable (https://github.com/settings/tokens/new) with 'repo' scope"

    echo "Downloading ego script"
    curl -L "https://${GITHUB_TOKEN}@raw.githubusercontent.com/10gen/mms/master/scripts/ops_manager/ego" -o ego && chmod +x ego

    # provision evergreen host
    echo "Spawning new VM in Evergreen..."

    evg spawn ubuntu1804-build

    while ! evg list | grep "ubuntu1804-build"  | grep -qi "running"; do printf .; sleep 3; done

    host=$(evg list | awk ' {print $3} ' | cut -d "@" -f 2)

    ./ego seed "ubuntu@${host}"

    echo "Deploying Ops Manager"

    # todo move version to some configuration
    om_package="https://s3.amazonaws.com/mongodb-mms-build-onprem/c78b383d0ea57690d7a15d4ab0099820e1c3b35f/mongodb-mms_4.0.6.50308.20181204T1611Z-1_x86_64.deb"
    ./ego nohup "ubuntu@${host}" ego scenario_install_package_from_link "${om_package}" "http://${host}:9080"

    if ! ./ego tail "ubuntu@$host"; then
        # seems 'tail' throws an error after finish which is not relevant
        # another thing that is used in mms: './ego wait_for_open_port $host 9080'
        echo "WARN: Ego tail exited with non-zero code $?, but most of all everything is ok!"
    fi

    export OPS_MANAGER_HOST=${host}

    # add ssh host key to known hosts
    ssh-keyscan "$OPS_MANAGER_HOST" >> ~/.ssh/known_hosts

    rm ego

    title "Ops Manager is ready! Hostname: ${OPS_MANAGER_HOST}"
}

# Generates kube yaml configuration for the Operator from helm chart and deletes + installs the Operator to the Kubernetes cluster
redeploy_operator() {
    url="$1"
    version="${2:-latest}"
    ns="$3"
    watch_namespace="${4:-$ns}"
    pull_policy="${5:-Always}"
    managed_security_context="${6:-false}"
    timeout="${7:-1m}"

    echo "Installing the Operator (registry url: ${url}, version: ${version}, namespace: ${ns}) to ${CLUSTER_NAME:-'e2e cluster'}..."

    check_app "helm" "helm is not installed, run 'make prerequisites' to install all necessary software"
    check_app "timeout" "coreutils is not installed, call \"brew install coreutils\""

    helm template public/helm_chart \
            --set registry.repository="${url}"  \
            --set registry.pullPolicy="${pull_policy}" \
            --set operator.env=dev  \
            --set operator.version="${version}"  \
            --set operator.watchNamespace="${watch_namespace}" \
            --set namespace="${ns}"  \
            --set managedSecurityContext="${managed_security_context}" \
            > my-operator.yaml

    # note that we don't remove all objects from file as it contains CRDs as well (and they will be blocked if there are
    # existing mongodb resources)

    kubectl delete deployment mongodb-enterprise-operator &> /dev/null || true
    kubectl apply -f my-operator.yaml

    rm my-operator.yaml

    echo "Waiting until the Operator gets to Running state..."
    ( while ! kubectl -n "${ns}" get pods -l app=mongodb-enterprise-operator -o jsonpath="{.items[0].status.phase}" | grep -q "Running"; do printf .; sleep 1; done ) & pid=$!
    if wait_for_or_kill $pid 120; then
        echo "Operator started on time"
        kubectl -n "${ns}" get pods -l app=mongodb-enterprise-operator -o jsonpath="{.items[0].status.phase}"
    else
        echo "Operator didn't start"
    fi

    # In the end let's check again and print the state
    if ! kubectl -n "${ns}" get pods -l app=mongodb-enterprise-operator -o jsonpath="{.items[0].status.phase}" | grep -q "Running"; then
        error "Operator hasn't reached RUNNING state after ${timeout}. The full yaml configuration for the pod is:"
        kubectl -n "${ns}" get pods -l app=mongodb-enterprise-operator -o yaml

        title "Operator failed to start, exiting"
        exit 1
    fi
    echo ""

    title "The Operator successfully installed to the Kubernetes cluster"
}

# Checks minikube and starts if necessary
# (If any of "host", "kubelet" or "apiserver" are "Stopped" or "Error" or there is no "Running")
ensure_minikube() {
    if [[ ! $(minikube status) \
        || $(minikube status | grep -q "Stopped") \
        || $(minikube status | grep -q "Error") \
        || ! $(minikube status | grep "Running") ]]; then

        echo "Starting minikube (Kubernetes version 1.11.0) as it's not started..."
        # todo minikube parameters to configuration
        minikube start --kubernetes-version v1.11.0 --memory 5120
    fi
}

# Creates kops cluster and adds the support for dashboard
create_kops_cluster() {
    cluster_name="${1}"
    node_count=${2}
    node_volume_size=${3}
    node_size="${4}"
    master_size="${5}"
    zones="${6}"

    title "Creating kops cluster $cluster_name"

    check_env_var "KOPS_STATE_STORE" "Make sure you add \"export KOPS_STATE_STORE=s3://kube-om-state-store\" to your ~/.bashrc"

    [[ ! -f ~/.ssh/id_aws_rsa ]] && ssh-keygen -f ~/.ssh/id_aws_rsa && ssh-add ~/.ssh/id_aws_rsa

	echo "Make sure you use the latest version of kops (>= 1.11.0): 'brew upgrade kops'"

	kops create cluster \
		--node-count "$node_count" \
		--zones "${zones}" \
		--node-size "${node_size}" \
		--node-volume-size "${node_volume_size}" \
		--master-size="${master_size}" \
		--master-volume-size 16  \
		--kubernetes-version=v1.11.6 \
		--ssh-public-key=~/.ssh/id_rsa.pub \
		--authorization RBAC "${cluster_name}"

	kops create secret --name "${cluster_name}" sshpublickey admin -i ~/.ssh/id_rsa.pub

	kops update cluster "${cluster_name}" --yes

	echo "Waiting until kops cluster gets ready..."

	timeout "20m" bash -c 'while ! kops validate cluster $cluster_name &>/dev/null ; do printf .; sleep 5; done'

    title "Kops cluster $cluster_name is ready!"

    # see https://codeandunicorns.com/kubernetes-dashboard-kops/
    title "Adding support for Kubernetes dashboard"
    kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/master/aio/deploy/recommended/kubernetes-dashboard.yaml

    #To create a service account with access to default namespace
    kubectl create serviceaccount dashboard -n default

    #To create a cluster role bind. Connecting service account and cluster level access
    kubectl create clusterrolebinding dashboard-admin -n default \
     --clusterrole=cluster-admin \
     --serviceaccount=default:dashboard

    title "Kubernetes dashboard is installed, use \"make dashboard\" to open it"
}
# gathers the information about K8s objects and writes it to the specified file handler (can be the stdout)
dump_diagnostic_information() {
    dump_file=$1

    if ! kubectl get mst -n "${PROJECT_NAMESPACE}" 2>&1 | grep -q "No resources found"; then
        header "Mongodb Standalones" >> "$dump_file"
        kubectl get mst -o yaml -n "${PROJECT_NAMESPACE}"  >> "$dump_file"
    fi

    if ! kubectl get mrs -n "${PROJECT_NAMESPACE}" 2>&1 | grep -q "No resources found"; then
        header "Mongodb ReplicaSets" >> "$dump_file"
        kubectl get mrs -o yaml -n "${PROJECT_NAMESPACE}"  >> "$dump_file"
    fi

    if ! kubectl get msc -n "${PROJECT_NAMESPACE}" 2>&1 | grep -q "No resources found"; then
        header "Mongodb ShardedClusters" >> "$dump_file"
        kubectl get msc -o yaml -n "${PROJECT_NAMESPACE}"  >> "$dump_file"
    fi

    header "All namespace objects" >> "$dump_file"
    kubectl get all -n "${PROJECT_NAMESPACE}" >> "$dump_file"

    if ! kubectl get pvc -n "${PROJECT_NAMESPACE}" 2>&1 | grep -q "No resources found"; then
        header "Persistent Volume Claims" >> "$dump_file"
        kubectl get pvc -o yaml -n "${PROJECT_NAMESPACE}"  >> "$dump_file"
    fi

    # we don't output pods for the operator and tests (they will be output if they didn't manage to start)
    for pod in $(kubectl get pods -n "${PROJECT_NAMESPACE}" -o name | grep -v mongodb-enterprise-operator); do
        header "$pod" >> "$dump_file"
        # in general describe produces shorter output than "get -o yaml" and seems it is enough to diagnose the problem
        kubectl describe "${pod}" -n "${PROJECT_NAMESPACE}"  >> "$dump_file"
    done
}
