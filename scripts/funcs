#!/usr/bin/env bash

# Set of common functions used by different scripts (evergreen, dev, build)
# Important: all the locations in the functions are relative to the root of the project so the calling
# script must make sure to cd there

fatal() {
    error "$1"
    exit 1
}

error() {
    echo "(!!) $1"
    return
}

title() {
    echo "=> $1"
}

header() {
    echo
    echo "--------------------------------------------------"
    echo "$1"
    echo "--------------------------------------------------"
}
check_env_var() {
    var_name="$1"
    msg="$2"
    set +u
    if [[ -z "${!var_name}" ]]; then
        echo "${msg}"
        exit 1
    fi
}

check_app() {
    var="$1"
    msg="$2"
    if ! which "${var}" > /dev/null; then
        echo "${msg}"
        exit 1
    fi
}

ensure_namespace() {
    ns="$1"
    if [[ -z "$(kubectl get ns | grep ${ns})" ]]; then
        kubectl create ns "${ns}"
    fi
}
# the function installs Ops Manager into Kubernetes cluster if it's not installed already
# Note, that this requires the k8s node with 4 Gb of free memory
ensure_ops_manager_k8s() {
    echo "##### Installing Ops Manager into Kubernetes..."

    if is_ops_manager_down; then
        echo "Ops Manager is not installed in this cluster. Doing it now."
        echo "Run \"make -C docker/mongodb-enterprise-ops-manager IMAGE_VERSION=<version> push\" to update Ops Manager image in ECR if necessary"
        echo "Also you'll need to update \"opsManager.version\" in scripts/evergreen/deployments/values-ops-manager.yaml"

        # Building Ops Manager image if we want to run it in local Kubernetes cluster
        if [[ ${REPO_TYPE:-'ecr'} = "local" ]]; then
            make -C docker/mongodb-enterprise-ops-manager build
        fi
        kubectl create namespace operator-testing  || true

        # Install Ops Manager
        helm template -f scripts/evergreen/deployments/values-ops-manager.yaml \
            docker/mongodb-enterprise-ops-manager/helm_chart > mongodb-enterprise-ops-manager.yaml

        kubectl delete -f mongodb-enterprise-ops-manager.yaml
        kubectl apply -f mongodb-enterprise-ops-manager.yaml

        rm mongodb-enterprise-ops-manager.yaml

        echo "Waiting until Statefulset for Ops Manager is running..."

        timeout "1m" bash -c 'while [[ $(kubectl -n operator-testing get statefulset mongodb-enterprise-ops-manager -o jsonpath="{.status.currentReplicas}") != "1" ]]; do sleep 1; done'

        echo "Statefulset is ready, waiting for the Ops Manager to start..."

        # We can't communicate with Ops Manager if it is inside Kubernetes, so we just
        # wait for this command to succeed.
        # Note, that for OM check for "status.phase==Running" may not work - we need to check the container status
        timeout "5m" bash -c 'while ! kubectl get pod/mongodb-enterprise-ops-manager-0 -o jsonpath="{.status.containerStatuses[0].ready}" -n operator-testing | grep -q "true"; do sleep 4; done'

        if ! kubectl get pod/mongodb-enterprise-ops-manager-0 -o jsonpath="{.status.containerStatuses[0].ready}" -n operator-testing | grep -q "true"; then
            error "Ops Manager hasn't started!"
            kubectl describe pod/mongodb-enterprise-ops-manager-0 -n operator-testing

            exit 1
        fi
        echo "Pod is ready, waiting for the admin to get registered..."

        # waiting until the om file is ready
        timeout "4m" bash -c 'while ! kubectl -n operator-testing exec mongodb-enterprise-ops-manager-0 ls "/opt/mongodb/mms/env/.ops-manager-env" &>/dev/null; do sleep 2; done'

        if ! kubectl -n operator-testing exec mongodb-enterprise-ops-manager-0 ls "/opt/mongodb/mms/env/.ops-manager-env" &>/dev/null; then
            error "No env file found in Ops Manager!"
            # todo print logs when OM image is fixed and rebuilt
            # kubectl logs  -n operator-testing mongodb-enterprise-ops-manager-0

            exit 1
        fi

        # we try to open ports for both security groups - in kops and openshift clusters
        ensure_vpc_rules "openshift-test-workersecgroup"
        ensure_vpc_rules "nodes.e2e"

        echo "Ops Manager is installed in this cluster. A new user will be added for automated tests to run."
    else
        echo "Ops Manager is already installed in this cluster."
        echo "If you want to start with a fresh Ops Manager installation, please delete the \"operator-testing\" namespace."
    fi

    echo "##### Ops Manager is ready"
    external_ip="$(kubectl get nodes -o wide | grep $(kubectl get pods/mongodb-enterprise-ops-manager-0 -n operator-testing -o wide | tail -n 1 | awk '{print $7}') | awk '{print $7}')"
    echo "Use the following address to access Ops Manager from the browser: http://${external_ip}:30039"

}

ensure_vpc_rules() {
    prefix="$1"
    group_id=$(aws ec2 describe-security-groups | jq -r '.SecurityGroups[] | select(.GroupName | startswith( "'"$prefix"'")) | .GroupId')

    if [[ -z $group_id ]]; then
        echo "Failed to find group id for EC2 security group with prefix $prefix"
        return 1
    fi
    # note, that the attempt to add the same rule will result in "already exists" error - so we just ignore errors by now
    aws ec2 authorize-security-group-ingress --group-id ${group_id} --protocol tcp --port 30039 --cidr "0.0.0.0/0" || true

    echo "Opened port 30039 in AWS for the security group with prefix $prefix (group id: $group_id)"
}
# returns true if the namespace doesn't exist or if the OM pod is not running and this lasts for more than or equal to 1 hour.
# This is supposed to work for concurrent builds when one of the builds is starting Ops Manager (ideally we need to check for 10 minutes
# but parsing dates in bash is hard :( )
is_ops_manager_down() {
    # todo ideally we need to check for how long OM was down - if less than 5 minutes - then it may seem that it's being started
    # by another build
    # unfortunately "[[ $(kubectl get pod/mongodb-enterprise-ops-manager-0 -n operator-testing | tail -n 1 | awk '{ print $NF }' | sed 's/h//') -ge 1 ]])"
    # doesn't work here as time can be returned in hours and minutes ("2h" vs "57m")
 ! kubectl get namespace/operator-testing &> /dev/null || \
    kubectl get pod/mongodb-enterprise-ops-manager-0 -o jsonpath="{.status.containerStatuses[0].ready}" -n operator-testing | grep -q "false"

}
# the function installs Ops Manager into Evergreen host via ego
ensure_ops_manager_evg() {
    title "Installing Ops Manager into Evergreen using ego"

    check_app "evg" \
                "evg script is not installed or you need to switch virtual environment, follow instruction at https://wiki.corp.mongodb.com/display/MMS/Ops+Manager+Release+setup+guide#OpsManagerReleasesetupguide-First-timeonly to make it work";

    # TODO this check can definitely be improved (walk through ubuntu1804-build hosts and find installed OMs)
    if evg list | grep "ubuntu1804-build" | grep -qi "running"; then
        export OPS_MANAGER_HOST=$(evg list | grep "ubuntu1804-build" | grep -i "running" | head -n 1 | awk ' {print $3} ' | cut -d "@" -f 2)
        echo "There is an Evergreen instance (${OPS_MANAGER_HOST}) running already - skipping ego installaton"
        return
    fi

    # download ego script
    check_env_var "GITHUB_TOKEN" "To download ego you need to specify the 'GITHUB_TOKEN' environment variable (https://github.com/settings/tokens/new) with 'repo' scope"

    echo "Downloading ego script"
    curl -L "https://${GITHUB_TOKEN}@raw.githubusercontent.com/10gen/mms/master/scripts/ops_manager/ego" -o ego && chmod +x ego

    # provision evergreen host
    echo "Spawning new VM in Evergreen..."

    evg spawn ubuntu1804-build

    while ! evg list | grep "ubuntu1804-build"  | grep -qi "running"; do printf .; sleep 3; done

    host=$(evg list | awk ' {print $3} ' | cut -d "@" -f 2)

    ./ego seed "ubuntu@${host}"

    echo "Deploying Ops Manager"

    # todo move version to some configuration
    om_package="https://s3.amazonaws.com/mongodb-mms-build-onprem/c78b383d0ea57690d7a15d4ab0099820e1c3b35f/mongodb-mms_4.0.6.50308.20181204T1611Z-1_x86_64.deb"
    ./ego nohup "ubuntu@${host}" ego scenario_install_package_from_link "${om_package}" "http://${host}:9080"

    if ! ./ego tail "ubuntu@$host"; then
        # seems 'tail' throws an error after finish which is not relevant
        # another thing that is used in mms: './ego wait_for_open_port $host 9080'
        echo "WARN: Ego tail exited with non-zero code $?, but most of all everything is ok!"
    fi

    export OPS_MANAGER_HOST=${host}

    rm ego

    title "Ops Manager is ready! Hostname: ${OPS_MANAGER_HOST}"
}

# Generates kube yaml configuration for the Operator from helm chart and deletes + installs the Operator to the Kubernetes cluster
redeploy_operator() {
    url="$1"
    version="${2:-latest}"
    ns="$3"
    pull_policy="${4:-Always}"
    managed_security_context="${5:-false}"
    timeout="${6:-1m}"

    echo "Installing the Operator (registry url: ${url}, version: ${version}, namespace: ${ns}) to ${CLUSTER_NAME}..."

    check_app "helm" "helm is not installed, run 'make prerequisites' to install all necessary software"
    check_app "timeout" "coreutils is not installed, call \"brew install coreutils\""

    helm template public/helm_chart \
            --set registry.repository=${url}  \
            --set registry.pullPolicy=${pull_policy} \
            --set operator.env=dev  \
            --set operator.version=${version}  \
            --set namespace=${ns}  \
            --set managedSecurityContext="${managed_security_context}" \
            > my-operator.yaml

    # note that we don't remove all objects from file as it contains CRDs as well (and they will be blocked if there are
    # existing mongodb resources)
    kubectl delete deployment mongodb-enterprise-operator || true
    kubectl apply -f my-operator.yaml
    rm my-operator.yaml

    echo "Waiting until the Operator gets to Running state..."

    timeout ${timeout} bash -c 'while ! kubectl -n '"${ns}"' get pods -l app=mongodb-enterprise-operator -o jsonpath="{.items[0].status.phase}" | grep -q "Running" ; do printf .; sleep 1; done'

    # In the end let's check again and print the state
    if ! kubectl -n "${ns}" get pods -l app=mongodb-enterprise-operator -o jsonpath="{.items[0].status.phase}" | grep -q "Running"; then
        error "Operator hasn't reached RUNNING state after ${timeout}. The full yaml configuration for the pod is:"
        kubectl -n "${ns}" get pods -l app=mongodb-enterprise-operator -o yaml

        title "Operator failed to start, exiting"
        exit 1
    fi
    echo ""

    title "The Operator successfully installed to the Kubernetes cluster"
}

# Checks minikube and starts if necessary
# (If any of "host", "kubelet" or "apiserver" are "Stopped" or "Error" or there is no "Running")
ensure_minikube() {
    if [[ ! $(minikube status) \
        || $(minikube status | grep -q "Stopped") \
        || $(minikube status | grep -q "Error") \
        || ! $(minikube status | grep "Running") ]]; then

        echo "Starting minikube (Kubernetes version 1.11.0) as it's not started..."
        # todo minikube parameters to configuration
        minikube start --kubernetes-version v1.11.0 --memory 5120
    fi
}

# gathers the information about K8s objects and writes it to the specified file handler (can be the stdout)
dump_diagnostic_information() {
    dump_file=$1

    header "All namespace objects" > $dump_file
    kubectl get all -n ${PROJECT_NAMESPACE} >> $dump_file

    if ! kubectl get pvc -n ${PROJECT_NAMESPACE} 2>&1 | grep -q "No resources found"; then
        header "Persistent Volume Claims" >> $dump_file
        kubectl get pvc -o yaml -n ${PROJECT_NAMESPACE}  >> $dump_file
    fi

    if ! kubectl get mst -n ${PROJECT_NAMESPACE} 2>&1 | grep -q "No resources found"; then
        header "Mongodb Standalones" >> $dump_file
        kubectl get mst -o yaml -n ${PROJECT_NAMESPACE}  >> $dump_file
    fi

    if ! kubectl get mrs -n ${PROJECT_NAMESPACE} 2>&1 | grep -q "No resources found"; then
        header "Mongodb ReplicaSets" >> $dump_file
        kubectl get mrs -o yaml -n ${PROJECT_NAMESPACE}  >> $dump_file
    fi

    if ! kubectl get msc -n ${PROJECT_NAMESPACE} 2>&1 | grep -q "No resources found"; then
        header "Mongodb ShardedClusters" >> $dump_file
        kubectl get msc -o yaml -n ${PROJECT_NAMESPACE}  >> $dump_file
    fi

    # we don't output pods for the operator and tests (they will be output if they didn't manage to start)
    for pod in $(kubectl get pods -n ${PROJECT_NAMESPACE} -o name | grep -v mongodb-enterprise-operator); do
        header "$pod" >> $dump_file
        # in general describe produces shorter output than "get -o yaml" and seems it is enough to diagnose the problem
        kubectl describe ${pod} -n ${PROJECT_NAMESPACE}  >> $dump_file
    done
}
