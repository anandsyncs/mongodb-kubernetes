#!/usr/bin/env bash


# shellcheck disable=SC1091
source scripts/funcs/printing

dump_objects() {
    local objects=$1
    local msg=$2
    local action=${3:-get -o yaml}

    if ! kubectl get "${objects}" -n "${PROJECT_NAMESPACE}" &> /dev/null; then
        return
    fi

    header "${msg}"
    # shellcheck disable=SC2086
    kubectl -n "${PROJECT_NAMESPACE}" ${action} "${objects}"
}

dump_all () {
    [[ "${MODE-}" = "dev" ]] && return

    # 1. Dump diagnostic information
    # gathers the information about K8s objects and writes it to the file which will be attached to Evergreen job
    mkdir -p logs

    dump_file="logs/diagnostics.txt"

    {
        dump_objects "mdb" "MongoDB Resources"
        dump_objects "mdbu" "MongoDBUser Resources"
        dump_objects "om" "MongoDBOpsManager Resources"
    } >> "$dump_file"

    {
        header "All namespace objects"
        kubectl get all -n "${PROJECT_NAMESPACE}"
        dump_objects "pvc" "Persistent Volume Claims"
        dump_objects "sts" "StatefulSets" describe
        dump_objects "validatingwebhookconfigurations" "Validating Webhook Configurations"
    } >> "$dump_file"

    dump_objects certificates.cert-manager.io "Cert-manager certificates" >> "${dump_file}"

    {
        header "All test/namespace CSR";
        kubectl get csr -o wide | grep "${PROJECT_NAMESPACE}" ;
        echo "Cluster currently has $(kubectl get csr -o name | wc -l) certificates"
    } >> "$dump_file"

    {
        kubectl describe $(kubectl get crd -o name | grep mongodb.com) > logs/crd.log
    }

    # we don't output pods for the operator and tests (they will be output if they didn't manage to start)
    for pod in $(kubectl get pods -n "${PROJECT_NAMESPACE}" -o name | grep -v "${OPERATOR_NAME:=mongodb-enterprise-operator}"); do
        header "$pod" >> "$dump_file"
        # in general describe produces shorter output than "get -o yaml" and seems it is enough to diagnose the problem
        kubectl describe "${pod}" -n "${PROJECT_NAMESPACE}"  >> "$dump_file"
    done

    # 2. Pods logs
    i=1
    if ! kubectl get pods -n "${PROJECT_NAMESPACE}" 2>&1 | grep -q "No resources found"; then
        for pod in $(kubectl get pods -n "${PROJECT_NAMESPACE}"  -o name | cut -d "/" -f 2 | grep -v "operator-"); do
            if kubectl exec ${pod} -n "${PROJECT_NAMESPACE}" -- ls /var/log/mongodb-mms-automation/automation-agent-verbose.log &>/dev/null; then
                echo "Writing agent and mongodb logs for pod ${pod} to logs"
                kubectl cp "${PROJECT_NAMESPACE}/${pod}:var/log/mongodb-mms-automation/automation-agent-verbose.log" "logs/${pod}-agent.log" &> /dev/null
                kubectl cp "${PROJECT_NAMESPACE}/${pod}:var/log/mongodb-mms-automation/mongodb.log" "logs/${pod}-mongodb.log" &> /dev/null
                # note that this file may get empty if the logs have already grew too much - seems it's better to have it explicitly empty then just omit
                kubectl logs -n "${PROJECT_NAMESPACE}" "${pod}" | jq -c -r 'select( .logType == "agent-launcher-script") | .contents' > "logs/${pod}-launcher.log"
            else
                echo "Writing log file for pod ${pod} to logs/${pod}.log"
                kubectl logs -n "${PROJECT_NAMESPACE}" "${pod}" > "logs/${pod}.log"
            fi

            # kubectl cp won't create any files if the file doesn't exist in the container
            agent_health_status="logs/${pod}-agent-health-status.json"
            kubectl cp "${PROJECT_NAMESPACE}/${pod}:var/log/mongodb-mms-automation/agent-health-status.json" "${agent_health_status}" &> /dev/null
            [ -f "${agent_health_status}" ] && jq < "logs/${pod}-agent-health-status.json" > tmpfile && mv tmpfile "${agent_health_status}"

            # cluster-config.json is a mounted volume and the actual file is located in the "..data" directory
            pod_cluster_config="logs/${pod}-cluster-config.json"
            kubectl cp "${PROJECT_NAMESPACE}/${pod}:var/lib/mongodb-automation/..data/cluster-config.json" "${pod_cluster_config}" &> /dev/null
            [ -f "${pod_cluster_config}" ] && jq < "logs/${pod}-cluster-config.json" > tmpfile && mv tmpfile "${pod_cluster_config}"

            kubectl cp "${PROJECT_NAMESPACE}/${pod}:var/log/mongodb-mms-automation/readiness.log" "logs/${pod}-readiness.log" &> /dev/null
            kubectl cp "${PROJECT_NAMESPACE}/${pod}:data/automation-mongod.conf" "logs/${pod}-automation-mongod.conf" &> /dev/null
            ((i++))
        done
    fi
    echo "logs for ${i} pods were written."
}
