#!/usr/bin/env bash

DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
cd "$(${DIR}/../gitroot)"

source scripts/funcs

##
## Does some preliminary actions to prepare testing environment:
## - prunes namespaces
## - starts Ops Manager if it's not started yet
##

# cleanup_env removes old namespaces and Ops Manager if there are too many of them
cleanup_env() {
    ns_count=$(kubectl get ns -o name | grep a- --count)
    echo "Current number of test namespaces: $ns_count"

    echo "##### Removing FAILED Persistent Volumes if any"
    # Note, that these volumes will be removed from EBS eventually (during a couple of hours), most of all they are stuck
    # in attaching - this results in nodes getting taints "NoSchedule"
    for v in $(kubectl get pv -o=custom-columns=NAME:.metadata.name,status:.status.phase | grep Failed | awk '{ print $1 }'); do
        kubectl delete pv ${v}
    done

    echo "##### Detaching the EBS Volumes that are stuck"
    for v in $(aws ec2 describe-volumes --filters Name=attachment.status,Values=attaching | grep VolumeId | cut -d "\"" -f 4); do
        set -v
        aws ec2 detach-volume --volume-id ${v} --force || true
        set +v
    done

    echo "##### Removing ESB volumes which are not used any more"
    # Seems Openshift (sometimes?) doesn't remove the volumes but marks them as available - we need to clean these volumes
    # manually
    for v in $(aws ec2 describe-volumes --filters Name=status,Values=available | grep VolumeId | cut -d "\"" -f 4); do
        set -v
        aws ec2 delete-volume --volume-id ${v} || true
        set +v
    done

    echo "##### Cleaning ECR repositories if necessary"
    check_and_clean_repo "dev/mongodb-enterprise-database"
    check_and_clean_repo "dev/mongodb-enterprise-operator"
    check_and_clean_repo "dev/mongodb-enterprise-tests"

}

# Update: taints are supposed not to appear as "cleanup_env" function is supposed to remove failed PVs, but let's keep
# this function for some time here
#
# sometimes in kops cluster some nodes get this taint that makes nodes non-schedulable. Just going over all nodes and
# trying to remove the taint is supposed to help
fix_taints() {
    for n in $(kubectl get nodes -o name); do
        kubectl taint nodes ${n} NodeWithImpairedVolumes:NoSchedule- 2> /dev/null || true
    done
}

# The functions checks the size for the ECR repository and recreates it if necessary
# This may affect some parallel running tests, so should be called quite rarely - when the size exceeds 500
# (note, that 'aws ecr list-images' for some reasons returns the ~700+ images when in fact there are 1000 of them -
# so we choose 500 as a some representative number)
check_and_clean_repo() {
    if [[ $(aws ecr list-images --repository-name "$1" | jq '.[] | length') -gt 500 ]]; then
        echo "Recreating the \"$1\" repo"
        aws ecr delete-repository --repository-name "$1" --force
        aws ecr create-repository --repository-name "$1"
    fi
}

fix_taints

cleanup_env


ops_manager_namespace="${1}"
ops_manager_version="${2}"
node_port="${3}"
# Not required when running against the Ops Manager Kubernetes perpetual instance
if [[ "${OM_EXTERNALLY_CONFIGURED:-}" != "true" ]]; then
    ensure_ops_manager_k8s ${ops_manager_namespace} ${ops_manager_version} ${node_port}
fi
