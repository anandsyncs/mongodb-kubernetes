#!/usr/bin/env bash

##
## Does some preliminary actions to prepare testing environment:
## - prunes namespaces
## - starts Ops Manager if it's not started yet
##

# cleanup_env removes old namespaces and Ops Manager if there are too many of them
cleanup_env() {
    MAX_NS_COUNT=50
    ns_count=$(kubectl get ns -o name | grep a- --count)
    echo "Current number of test namespaces: $ns_count"

    if [[ ns_count -gt ${MAX_NS_COUNT} ]]; then
        echo "##### Cleaning namespaces and Ops Manager as number of namespaces exceeds $MAX_NS_COUNT"

        for f in $(kubectl get ns -o name | grep -E "^namespaces/a-"); do
            # temp until CLOUDP-35555 is resolved
            for m in $(kubectl get mrs -n ${f} -o name); do
                kubectl patch  ${m}  --type json   -p='[{"op": "remove", "path": "/metadata/finalizers"}]' -n ${f}
            done
            kubectl delete ${f};
        done

        kubectl delete operator-testing

        echo "##### Removed old namespaces and Ops Manager one"
    fi

    echo "##### Removing FAILED Persistent Volumes if any"
    # Note, that these volumes will be removed from EBS eventually (during a couple of hours), most of all they are stuck
    # in attaching - this results in nodes getting taints "NoSchedule"
    for v in $(kubectl get pv -o=custom-columns=NAME:.metadata.name,status:.status.phase | grep Failed | awk '{ print $1 }'); do
        kubectl delete pv ${v}
    done

    echo "##### Detaching the EBS Volumes that are stuck"
    for v in $(aws ec2 describe-volumes --filters Name=attachment.status,Values=attaching | grep VolumeId | cut -d "\"" -f 4); do
        set -v
        aws ec2 detach-volume --volume-id ${v} --force || true
        set +v
    done

    echo "##### Removing ESB volumes which are not used any more"
    # Seems Openshift (sometimes?) doesn't remove the volumes but marks them as available - we need to clean these volumes
    # manually
    for v in $(aws ec2 describe-volumes --filters Name=status,Values=available | grep VolumeId | cut -d "\"" -f 4); do
        set -v
        aws ec2 delete-volume --volume-id ${v} || true
        set +v
    done

}

# Update: taints are supposed not to appear as "cleanup_env" function is supposed to remove failed PVs, but let's keep
# this function for some time here
#
# sometimes in kops cluster some nodes get this taint that makes nodes non-schedulable. Just going over all nodes and
# trying to remove the taint is supposed to help
fix_taints() {
    for n in $(kubectl get nodes -o name); do
        kubectl taint nodes ${n} NodeWithImpairedVolumes:NoSchedule- 2> /dev/null || true
    done
}

# ensure_ops_manager installs Ops Manager if it's not installed already
ensure_ops_manager() {
    echo "##### Installing Ops Manager..."

    OPERATOR_TESTING_FRAMEWORK_NS=operator-testing
    if ! kubectl get namespace/${OPERATOR_TESTING_FRAMEWORK_NS} &> /dev/null; then
        echo "Ops Manager is not installed in this cluster. Doing it now."
        # If Ops Manager is not running, run it first!
        kubectl create namespace "${OPERATOR_TESTING_FRAMEWORK_NS}"

        # Install Ops Manager
        helm template -f deployments/values-ops-manager.yaml \
            ../../docker/mongodb-enterprise-ops-manager/helm_chart > mongodb-enterprise-ops-manager.yaml || exit 1

        echo "------------------------------------------------------"
        echo "Ops Manager objects to be created:"
        echo "------------------------------------------------------"
        cat mongodb-enterprise-ops-manager.yaml

        kubectl apply -f mongodb-enterprise-ops-manager.yaml  || exit 1

        echo "Waiting until Ops Manager is running..."

        # Just checking for 'running' may show false positive:
        # kubectl --namespace operator-testing get pods/mongodb-enterprise-ops-manager-0
        # NAME                               READY     STATUS    RESTARTS   AGE
        # mongodb-enterprise-ops-manager-0   0/1       Running   117        14h
        while ! kubectl --namespace "${OPERATOR_TESTING_FRAMEWORK_NS}" get pods/mongodb-enterprise-ops-manager-0 | grep -q '0/1'; do sleep 4; done

        # wait for ops manager to really start
        echo "Ops Manager container is in Running state, waiting for Ops Manager to start."
        # We can't communicate with Ops Manager if it is inside Kubernetes, so we just
        # wait for this command to succeed.
        while ! kubectl --namespace "${OPERATOR_TESTING_FRAMEWORK_NS}" get pods/mongodb-enterprise-ops-manager-0 -o jsonpath="{.status.containerStatuses[0].ready}" | grep -q "true"; do sleep 4; done

        echo "Ops Manager is installed in this cluster. A new user will be added for automated tests to run."
        sleep 10 # sleep for a few seconds so the user has time to be created.
    else
        echo "Ops Manager is already installed in this cluster."
        echo "If you want to start with a fresh Ops Manager installation, please delete the ${OPERATOR_TESTING_FRAMEWORK_NS} namespace."
    fi

    echo "##### Ops Manager is ready"
}

cleanup_env


ensure_ops_manager
