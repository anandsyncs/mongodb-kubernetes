ignore:
  - "*.md"
  - "public/support/*"
  - "public/samples/*"
stepback: true
exec_timeout_secs: 1800 # 30 minutes is the longest we'll ever run
variables:
  - &ops_manager_40_first
    ops_manager_version: "4.0.11.50485.20190502T1847Z-1_test"
    ops_manager_namespace: "operator-testing-40-first"
    node_port: 30041
  - &ops_manager_42_current
    ops_manager_version: "4.2.0.56423.20190722T1139Z-1_test"
    ops_manager_namespace: "operator-testing-42-current"
    node_port: 30043

# TODO: CLOUDP-42158 there are some incompatibilities with the operator and 4.1.x Ops Managers
#  - &ops_manager_41_current
#    ops_manager_version: "4.1.6.54491.20190508T1510Z-1_test"
#    ops_manager_namespace: "operator-testing-41-current"
#    node_port: 30042

  - &cloud_manager_qa
    ops_manager_version: "cloud_qa"

  # These need to be defined manually.
  # TODO(rodrigo): This will be improved as part of CLOUDP-38550.
  - &rhel_prebuilt_images
    prebuilt_tag: 1.0
    prebuilt_database_name: mdb-database
    prebuilt_operator_name: mdb-operator
    prebuilt_registry: quay.io/some
    prebuilt_images_run: true

    # Openshift v3.11 Testing Environment
  - &kubernetes_environment_openshift_3_11:
    kube_environment_name: openshift_3_11

    # Kops Vanilla Kubernetes v1.11
  - &kubernetes_environment_vanilla_1_11:
    kube_environment_name: vanilla_1_11

functions:
  "golint":
    - command: shell.exec
      params:
        working_dir: src/github.com/10gen/ops-manager-kubernetes
        script: |
          WORKDIR=${workdir} ./scripts/evergreen/lint_code.sh
  "clone":
    - command: shell.exec
      params:
        script: |
          mkdir -p src/github.com/10gen
    - command: git.get_project
      type: system
      params:
        directory: src/github.com/10gen/ops-manager-kubernetes

  "test_operator":
    - command: shell.exec
      type: system
      params:
        working_dir: src/github.com/10gen/ops-manager-kubernetes
        script: |
          export WORKDIR=${workdir}
          CONTINUE=true ./scripts/evergreen/build_operator

  "build_operator":
    - command: shell.exec
      type: system
      params:
        shell: bash
        working_dir: src/github.com/10gen/ops-manager-kubernetes
        script: |
          export WORKDIR=${workdir}

          if [[ "${task_name}" = "release_operator_rhel_connect" ]] || [[ "${task}" = "release_operator" ]]; then
            # Make sure release matches a correct release name.

            if [[ -z $(git status --short) ]]; then
              echo "The repository is dirty. Make sure no uncommited or modified files were added to the patch."
              git status --short
              exit 1
            fi

            release_version="$(git describe --dirty)"
            if ! [[ "$release_version" =~ ^[0-9]{1,2}\.[0-9]{1,2}$ ]]; then
              echo "$release_version does not conform to what we expect as a version label."
              exit 1
            fi

            echo "Proceeding with release $release_version"
          fi

          SKIP_TESTING=true ./scripts/evergreen/build_operator

    - command: s3.put
      params:
        aws_key: ${mms_build_s3_aws_access_key}
        aws_secret: ${mms_build_s3_aws_secret}
        local_file: src/github.com/10gen/ops-manager-kubernetes/docker/mongodb-enterprise-operator/content/mongodb-enterprise-operator
        remote_file: ops-manager-operator/${revision}/mongodb-enterprise-operator
        bucket: ops-manager-kubernetes-build
        permissions: public-read
        content_type: application/octet-stream

  "upload_e2e_logs":
    # todo note, that the bucket is public so far - there was something with permissions that didn't allow uploads for the eng test acccount
    - command: s3.put
      params:
        aws_key: ${mms_eng_test_aws_access_key}
        aws_secret: ${mms_eng_test_aws_secret}
        local_files_include_filter:
          - src/github.com/10gen/ops-manager-kubernetes/logs/*
        remote_file: logs/${task_id}/
        bucket: operator-e2e-artifacts
        permissions: public-read
        content_type: application/octet-stream

  # push the latest build of the operator to s3 for the RH build-service to be able to build the RHEL image.
  "push_operator_binary_for_rhel":
    - command: s3.put
      params:
        aws_key: ${mms_build_s3_aws_access_key}
        aws_secret: ${mms_build_s3_aws_secret}
        local_file: src/github.com/10gen/ops-manager-kubernetes/docker/mongodb-enterprise-operator/content/mongodb-enterprise-operator
        remote_file: ops-manager-operator/latest/mongodb-enterprise-operator
        bucket: ops-manager-kubernetes-build
        permissions: public-read
        content_type: application/octet-stream

  "build_rhel_images":
    - command: shell.exec
      params:
        working_dir: src/github.com/10gen/ops-manager-kubernetes
        script: |
          export PATH=${workdir}/bin:$PATH

          if [ "${is_patch}" != "true" ]; then
            echo "Releasing is done only from patches"
            exit 0
          fi

          set -euo

          release="$(jq --raw-output .mongodbOperator < release.json)"

          echo "Performing release $release!"
          scripts/evergreen/build_operator_rhel.sh $release ${rhc_operator_pid}
          scripts/evergreen/build_operator_rhel.sh $release ${rhc_database_pid}

  "setup_jq":
    - command: shell.exec
      params:
        shell: bash
        script: |
          echo "Downloading jq"
          curl -L https://github.com/stedolan/jq/releases/download/jq-1.6/jq-linux64 -o jq
          chmod +x jq
          mkdir -p ${workdir}/bin/
          mv jq ${workdir}/bin/
          echo "Installed jq to ${workdir}/bin/"

  "setup_aws":
    - command: shell.exec
      params:
        shell: bash
        script: |
            src/github.com/10gen/ops-manager-kubernetes/scripts/evergreen/setup_aws

  "setup_docker":
    - command: shell.exec
      params:
        shell: bash
        script: |
            src/github.com/10gen/ops-manager-kubernetes/scripts/evergreen/setup_docker

  download_kubectl:
    - command: shell.exec
      params:
        shell: bash
        script: |
          export WORKDIR=${workdir}
          export BINDIR=$WORKDIR/bin ; mkdir -p $BINDIR
          export PATH=$BINDIR:$PATH

          echo "Downloading kubectl"
          curl -s -LO https://storage.googleapis.com/kubernetes-release/release/v1.11.3/bin/linux/amd64/kubectl
          chmod +x kubectl
          mv kubectl $BINDIR

          echo "Downloading helm"
          HELM=helm.tgz
          curl -s https://storage.googleapis.com/kubernetes-helm/helm-v2.10.0-linux-amd64.tar.gz --output $HELM
          tar xfz $HELM &> /dev/null
          mv linux-amd64/helm $BINDIR
          rm $HELM


  setup_kubernetes_environment:
  - command: shell.exec
    params:
      shell: bash
      script: |
          export WORKDIR=${workdir}
          export BINDIR=$WORKDIR/bin
          export PATH=$BINDIR:$PATH

          if [ "${kube_environment_name}" = "openshift_3_11" ]; then
            echo "Downloading OC & setting up Openshift cluster"
            OC_PKG=oc-linux.tar.gz
            curl -s -L https://github.com/openshift/origin/releases/download/v3.11.0/openshift-origin-client-tools-v3.11.0-0cbc58b-linux-64bit.tar.gz --output $OC_PKG
            tar xfz $OC_PKG &> /dev/null
            mv openshift-origin-client-tools-v3.11.0-0cbc58b-linux-64bit/oc $BINDIR

            echo "Setting up OpenShift variant"
            echo "Token: ${openshift_cluster_token}"
            oc login ${openshift_cluster_url} --token=${openshift_cluster_token} --insecure-skip-tls-verify
            kubectl config use-context default/master-openshift-cluster-mongokubernetes-com:8443/admin
          elif [ "${kube_environment_name}" = "vanilla_1_11" ]; then
            if [ ! -z ${cluster_name} ]; then
              export CLUSTER=${cluster_name}
            else
              export CLUSTER=e2e.mongokubernetes.com
            fi

            # AWS creds
            export AWS_ACCESS_KEY_ID=${mms_eng_test_aws_access_key}
            export AWS_SECRET_ACCESS_KEY=${mms_eng_test_aws_secret}
            export AWS_REGION=${mms_eng_test_aws_region}
            export AWS_DEFAULT_REGION=${mms_eng_test_aws_region}
            export KOPS_STATE_STORE=s3://kube-om-state-store

            echo "Downloading kops"
            curl -s -L https://github.com/kubernetes/kops/releases/download/1.11.1/kops-linux-amd64 -o kops
            chmod +x kops
            mv kops $BINDIR

            if ! kops get clusters | grep -q $CLUSTER; then
              echo "Cluster $CLUSTER not found, exiting..."
              echo run "make recreate-e2e-kops imsure=yes cluster=$CLUSTER"
              kops get clusters
              exit 1
            fi

            kops export kubecfg $CLUSTER

          else
            echo "kube_environment_name not recognized"
            echo "value is <<${kube_environment_name}>>. If empty it means it was not set"
          fi


  "gotest_parse_files":
    - command: gotest.parse_files
      params:
        files: ["*.suite", "src/**/*.suite"]

  "check_evergreen_health":
    - command: shell.exec
      params:
        working_dir: src/github.com/10gen/ops-manager-kubernetes
        script: |
          mkdir -p logs
          cat <<EOF >> logs/evergreen_environment.txt
          Checking Evergreen Host Health
          IS_PATCH=${is_patch}
          AUTHOR=${author}
          TASK_ID=${task_id}
          TASK_NAME=${task_name}
          EXECUTION=${execution}
          BUILD_ID=${build_id}
          BUILD_VARIANT=${build_variant}
          VERSION_ID=${version_id}
          WORKDIR=${workdir}
          REVISION=${revision}
          PROJECT=${project}
          BRANCH_NAME=${branch_name}
          DISTRO_ID=${distro_id}
          CREATED_AT=${created_at}
          REVISION_ORDER_ID=${revision_order_id}
          GITHUB_PR_NUMBER=${github_pr_number}
          GITHUB_ORG=${github_org}
          GITHUB_REPO=${github_repo}
          GITHUB_AUTHOR=${github_author}
          EOF

  "build_and_push_image":
    - command: shell.exec
      params:
        working_dir: src/github.com/10gen/ops-manager-kubernetes
        script: |
          set +x
          # Releases are performed only for patches
          if [ -n "${release_object}" ] && [ "${is_patch}" != "true" ]; then
            echo "Releasing is done only from patches"
            exit 0
          fi

          # AWS creds
          export AWS_ACCESS_KEY_ID=${mms_eng_test_aws_access_key}
          export AWS_SECRET_ACCESS_KEY=${mms_eng_test_aws_secret}

          # Quay prod creds
          export QUAY_PROD_USER=${quay_prod_username}
          export QUAY_PROD_PASSWORD=${quay_prod_robot_token}

          if [ "${tag_images_with_version}" = "true" ]; then
            export REVISION=${version_id}
          fi

          python3 -m venv venv
          source venv/bin/activate
          python3 -m pip install -r scripts/evergreen/requirements.txt

          # Always consider the caveat of "${var} vs $var" in evergreen!!
          docker_args="${docker_args}"

          # Releases override REVISION from predefined tags
          if [ -n "${release_object}" ]; then
            export REVISION=$(./scripts/evergreen/read_release_version.py --release-app ${release_object})

            # When releasing mongodb enterprise the docker tag and version of MongoDB are the same
            if [ "${release_object}" = "mongodbEnterprise" ]; then
              export docker_args="MONGO_VERSION=$REVISION"
            fi
          fi

          ./scripts/evergreen/build_and_push.py \
              --image ${image_name} \
              --tag "$REVISION" \
              --registry "${env}" \
              --path "${path}" \
              --docker-args "$docker_args" \
              $(if [ -n "${release_object}" ]; then echo '--with-latest-tag'; fi)

  # Populate the Ops Manager Kubernetes Perpetual instance, env vars
  "omkp_setup": &omkp_setup
    command: shell.exec
    params:
      working_dir: src/github.com/10gen/ops-manager-kubernetes
      shell: bash
      script: |
        if [[ "${omkp_enabled}" == "true" ]]; then
          echo "echo Using the Ops Manager Kubernetes Perpetual instance..."  > ${workdir}/.ops-manager-env
          echo "export OM_BASE_URL=${omkp_host}"                             >> ${workdir}/.ops-manager-env
          echo "export OM_USER=${omkp_user}"                                 >> ${workdir}/.ops-manager-env
          echo "export OM_API_KEY=${omkp_api_key}"                           >> ${workdir}/.ops-manager-env
          echo "export OM_EXTERNALLY_CONFIGURED=${omkp_enabled}"             >> ${workdir}/.ops-manager-env
          echo "Using the Ops Manager Kubernetes Perpetual Instance, env stored at '${workdir}/.ops-manager-env')"
        else
          echo "Skipping the Ops Manager Kubernetes Perpetual config (omkp_enabled=${omkp_enabled})"
        fi

  "configure_cloud_manager_qa": &configure_cloud_manager_qa
    command: shell.exec
    params:
      working_dir: src/github.com/10gen/ops-manager-kubernetes
      shell: bash
      script: |
        echo "Configuring Cloud Manager QA"
        if echo "${build_variant}" | grep -Eq "cloud_qa|multiple_namespace|openshift_rhel_prebuilt_images"; then
          echo "export OM_BASE_URL=${e2e_cloud_qa_baseurl}"      > ${workdir}/.ops-manager-env
          echo "export OM_USER=${e2e_cloud_qa_user}"            >> ${workdir}/.ops-manager-env
          echo "export OM_API_KEY=${e2e_cloud_qa_apikey}"       >> ${workdir}/.ops-manager-env
          echo "export OM_ORGID=${e2e_cloud_qa_orgid}"          >> ${workdir}/.ops-manager-env
          echo "export OM_EXTERNALLY_CONFIGURED=true"           >> ${workdir}/.ops-manager-env
        fi

  "e2e_test":
    - *omkp_setup
    - *configure_cloud_manager_qa
    - command: shell.exec
      params:
        working_dir: src/github.com/10gen/ops-manager-kubernetes
        script: |
          # revision is only supposed to be set in evergreen
          if [ "${prebuilt_images_run}" = "true" ]; then
            echo "Testing prebuilt images"
            export REVISION=${prebuilt_tag}
            export DATABASE_NAME=${prebuilt_database_name}
            export OPERATOR_NAME=${prebuilt_operator_name}
            export REGISTRY=${prebuilt_registry}
            export TEST_IMAGE_TAG=${version_id}
          else
            export REVISION=$(git rev-parse HEAD)
            export VERSION_ID=${version_id}
            export OPERATOR_NAME=mongodb-enterprise-operator
            export DATABASE_NAME=mongodb-enterprise-database
            export REGISTRY="268558157000.dkr.ecr.us-east-1.amazonaws.com/dev"
          fi

          export WORKDIR=${workdir}
          export IS_EVERGREEN=${revision}
          export PATH=${workdir}/bin:$PATH
          export MANAGED_SECURITY_CONTEXT="false"
          export BUILD_VARIANT=${build_variant}
          export TASK_ID=${task_id}
          export TASK_NAME=${task_name}
          export OPS_MANAGER_NAMESPACE=${ops_manager_namespace}
          export NODE_PORT=${node_port}

          export MMS_VERSION=${ops_manager_version}

          echo "Build Variant: ${build_variant}"
          set -e # fail on first failed test

          if echo "${build_variant}" | grep -q "openshift"; then
            echo "Setting up OpenShift variant"
            export MANAGED_SECURITY_CONTEXT="true"
          fi

          # Externally Configured Ops Manager (Perpetual Instance or Cloud Manager)
          test -f ${workdir}/.ops-manager-env && source ${workdir}/.ops-manager-env

          # A complete environment is set each time.
          WATCH_NAMESPACE=${watch_namespace} TASK_NAME=${task_name} STATIC_NAMESPACE=${static_namespace} ./scripts/evergreen/e2e_tests.sh

  "e2e_test_upgrade_operator":
    - *configure_cloud_manager_qa
    - command: shell.exec
      params:
        working_dir: src/github.com/10gen/ops-manager-kubernetes
        script: |
          # revision is only supposed to be set in evergreen
          export WORKDIR=${workdir}
          export IS_EVERGREEN=${revision}
          export PATH=${workdir}/bin:$PATH
          export REVISION=$(git rev-parse HEAD)
          export PATH=${workdir}/bin:$PATH
          export VERSION_ID=${version_id}
          export MANAGED_SECURITY_CONTEXT="true"
          export BUILD_VARIANT=${build_variant}
          export TASK_ID=${task_id}
          export TASK_NAME=${task_name}
          export OPS_MANAGER_NAMESPACE=${ops_manager_namespace}
          export NODE_PORT=${node_port}
          export OPERATOR_NAME=mongodb-enterprise-operator
          export DATABASE_NAME=mongodb-enterprise-database

          set -e # fail on first failed test

          # Externally Configured Ops Manager (Perpetual Instance or Cloud Manager)
          test -f ${workdir}/.ops-manager-env && source ${workdir}/.ops-manager-env

          # Will generate a random namespace to use during this run
          source scripts/funcs
          export PROJECT_NAMESPACE=$(generate_random_namespace)
          echo "Using $PROJECT_NAMESPACE"

          export CURRENT_VERSION=$(grep mongodbOperator release.json | awk '{ print $2 }' | cut -d '"' -f 2)
          echo "Setting up environment with current operator release: $CURRENT_VERSION"

          export REGISTRY="quay.io/mongodb"
          # By setting CURRENT_VERSION we are indicating the e2e_tests script to install this version
          # which is the latest released version, as defined in the release.json file.
          export OPERATOR_UPGRADE_IN_PROGRESS="stage1"
          TASK_NAME=e2e_operator_upgrade_build_deployment ./scripts/evergreen/e2e_tests.sh

          # Before running the next test, the CURRENT_VERSION variable is unset, then the latest
          # Version of the operator will be installed, effectively, updating the currently running operator
          unset CURRENT_VERSION
          export OPERATOR_UPGRADE_IN_PROGRESS="stage2"

          export REGISTRY="268558157000.dkr.ecr.us-east-1.amazonaws.com/dev"
          echo "Deploying the new Operator with version $VERSION_ID"
          TASK_NAME=e2e_operator_upgrade_scale_and_verify_deployment ./scripts/evergreen/e2e_tests.sh

  "prepare_test_env":
    - *omkp_setup
    - *configure_cloud_manager_qa
    - command: shell.exec
      params:
        working_dir: src/github.com/10gen/ops-manager-kubernetes/scripts/evergreen
        script: |
          export PATH=${workdir}/bin:$PATH
          export AWS_ACCESS_KEY_ID=${mms_eng_test_aws_access_key}
          export AWS_SECRET_ACCESS_KEY=${mms_eng_test_aws_secret}
          export AWS_REGION=${mms_eng_test_aws_region}
          export AWS_DEFAULT_REGION=${mms_eng_test_aws_region}

          test -f ${workdir}/.ops-manager-env && source ${workdir}/.ops-manager-env

          echo "Preparing Ops Manager ${ops_manager_version}"
          ./prepare_test_env ${ops_manager_namespace} ${ops_manager_version} ${node_port}

  "post_task":
  - command: shell.exec
    params:
      script: |
        if [ ! "$(which docker)" ]; then
          # if not docker present, no images either.
          exit 0
        fi

        echo "Removing all docker images in this machine"
        docker rmi -f $(docker images -a -q) &> /dev/null || true

        echo "Are there any remaining Docker images in this host?"
        docker images


post:
  - func: "post_task"

tasks:
- name: unit_tests
  tags: ["unit_tests"]
  commands:
    - func: "test_operator"
    - func: "gotest_parse_files"

- name: unit_tests_lint
  tags: ["unit_tests_lint"]
  commands:
    - func: "golint"

- name: release_operator
  tags: ["release_operator"]
  patch_only: true
  commands:
    - func: "clone"
    - func: "build_operator"
    - func: "setup_docker"
    - func: "build_and_push_image"
      vars:
        release_object: "mongodbOperator"
        env: production
        image_name: mongodb-enterprise-operator
    - func: "build_and_push_image"
      vars:
        release_object: "mongodbOperator"
        env: production
        image_name: mongodb-enterprise-database

- name: release_operator_rhel_connect
  tags: ["release_operator_rhel"]
  patch_only: true
  commands:
    - func: clone
    - func: build_rhel_images

- name: build_operator_rhel_connect
  tags: ["build_operator_rhel"]
  patch_only: true
  commands:
    - func: "clone"
    - func: "setup_jq"
    - func: "build_operator"
    - func: "push_operator_binary_for_rhel"

# Note that this task doesn't work so far as it needs different quay account and token
- name: release_mongodb_enterprise
  tags: ["release_mongodb_enterprise"]
  patch_only: true
  commands:
    - func: "clone"
    - func: "setup_docker"
    - func: "build_and_push_image"
      vars:
        release_object: "mongodbEnterprise"
        env: production
        image_name: mongodb-enterprise
        path: mongodb-enterprise/4.0

- name: build_images
  exec_timeout_secs: 600
  priority: 1
  commands:
    - func: clone
    - func: check_evergreen_health
    - func: setup_aws
    - func: setup_jq
    - func: setup_docker
    - func: build_operator
    - func: build_and_push_image
      vars:
        env: development
        image_name: mongodb-enterprise-operator
        tag_images_with_version: "true"
    - func: build_and_push_image
      vars:
        env: development
        image_name: mongodb-enterprise-database
        tag_images_with_version: "true"
    - func: build_and_push_image
      vars:
        env: development
        image_name: mongodb-enterprise-tests
        tag_images_with_version: "true"

- name: prepare_cluster_openshift_3_11
  exec_timeout_secs: 1200
  priority: 1
  commands:
    - func: clone
    - func: setup_aws
    - func: download_kubectl
    - func: setup_kubernetes_environment
      vars:
        kube_environment_name: openshift_3_11
    - func: setup_jq
    - func: prepare_test_env
      vars:
        <<: *ops_manager_42_current

- name: prepare_cluster_vanilla_1_11
  exec_timeout_secs: 1200
  priority: 1
  commands:
    - func: clone
    - func: setup_aws
    - func: setup_jq
    - func: download_kubectl
    - func: setup_kubernetes_environment
      vars:
        kube_environment_name: vanilla_1_11
    - func: prepare_test_env
      vars:
        <<: *ops_manager_40_first

    - func: prepare_test_env
      vars:
        <<: *ops_manager_42_current

- name: e2e_operator_upgrade
  exec_timeout_secs: 1200
  commands:
    - func: e2e_test_upgrade_operator

- name: e2e_standalone_schema_validation
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_replica_set_schema_validation
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_sharded_cluster_schema_validation
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_users_schema_validation
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_replica_set_different_namespaces
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"
      vars:
        watch_namespace: "*"

- name: e2e_standalone_config_map
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_standalone_groups
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_standalone_recovery
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_standalone_recovery_k8s
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_replica_set
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_replica_set_recovery
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_replica_set_config_map
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_replica_set_ent
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_replica_set_groups
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_replica_set_pv
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_replica_set_pv_multiple
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_replica_set_8_members
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_sharded_cluster
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_sharded_cluster_pv
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_sharded_cluster_recovery
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_sharded_cluster_secret
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_sharded_cluster_scale_shards
  exec_timeout_secs: 720
  commands:
    - func: "e2e_test"

- name: e2e_standalone_type_change_recovery
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_all_mongodb_resources_parallel
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_standalone_upgrade_downgrade
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_replica_set_upgrade_downgrade
  exec_timeout_secs: 900
  commands:
    - func: "e2e_test"

- name: e2e_sharded_cluster_upgrade_downgrade
  exec_timeout_secs: 900
  commands:
    - func: "e2e_test"

- name: e2e_standalone_no_tls_no_status_is_set
  exec_timeout_secs: 600
  commands:
  - func: "e2e_test"

- name: e2e_replica_set_tls_allow
  exec_timeout_secs: 600
  commands:
  - func: "e2e_test"

- name: e2e_replica_set_tls_prefer
  exec_timeout_secs: 600
  commands:
  - func: "e2e_test"

- name: e2e_replica_set_tls_require
  exec_timeout_secs: 1200
  commands:
  - func: "e2e_test"

- name: e2e_replica_set_tls_require_custom_ca
  exec_timeout_secs: 1200
  commands:
  - func: "e2e_test"

- name: e2e_sharded_cluster_tls_require_custom_ca
  exec_timeout_secs: 1200
  commands:
  - func: "e2e_test"

- name: e2e_tls_x509_sc_custom_ca
  exec_timeout_secs: 1200
  commands:
  - func: "e2e_test"

- name: e2e_replica_set_tls_require_and_disable
  exec_timeout_secs: 1200
  commands:
  - func: "e2e_test"

- name: e2e_tls_multiple_different_ssl_configs
  exec_timeout_secs: 1200
  commands:
  - func: "e2e_test"

- name: e2e_replica_set_tls_require_upgrade
  exec_timeout_secs: 1200
  commands:
  - func: "e2e_test"

- name: e2e_sharded_cluster_tls_require
  exec_timeout_secs: 1200
  commands:
  - func: "e2e_test"

- name: e2e_tls_x509_rs
  # longer timeout than usual as this test tests recovery from bad states which can take some time
  exec_timeout_secs: 1200
  commands:
  - func: "e2e_test"

- name: e2e_tls_x509_sc
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_tls_x509_users_addition_removal
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_tls_x509_user_connectivity
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_tls_x509_configure_all_options_rs
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_tls_x509_configure_all_options_sc
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

task_groups:
- name: unit_task_group
  setup_group:
    - func: "clone"
  tasks:
    - unit_tests
    - unit_tests_lint

- name: e2e_core_task_group
  max_hosts: 3
  setup_group:
    - func: clone
    - func: download_kubectl
    - func: setup_kubernetes_environment

  tasks:
    - e2e_all_mongodb_resources_parallel
    - e2e_standalone_config_map
    - e2e_standalone_groups
    - e2e_standalone_schema_validation
    - e2e_standalone_type_change_recovery
    - e2e_standalone_upgrade_downgrade
    - e2e_sharded_cluster_schema_validation
    - e2e_standalone_recovery
    - e2e_standalone_recovery_k8s
    - e2e_replica_set
    - e2e_replica_set_config_map
    - e2e_replica_set_ent
    - e2e_replica_set_groups
    - e2e_replica_set_recovery
    - e2e_replica_set_pv
    - e2e_replica_set_pv_multiple
    - e2e_replica_set_schema_validation
    - e2e_replica_set_upgrade_downgrade
    - e2e_replica_set_8_members
    - e2e_sharded_cluster
    - e2e_sharded_cluster_pv
    - e2e_sharded_cluster_recovery
    - e2e_sharded_cluster_secret
    - e2e_sharded_cluster_upgrade_downgrade
    - e2e_operator_upgrade
  teardown_task:
    - func: "upload_e2e_logs"

- name: e2e_tls_task_group
  max_hosts: 3
  setup_group:
    - func: clone
    - func: download_kubectl
    - func: setup_kubernetes_environment

  tasks:
    - e2e_sharded_cluster_tls_require
    - e2e_standalone_no_tls_no_status_is_set
    - e2e_replica_set_tls_allow
    - e2e_replica_set_tls_prefer
    - e2e_replica_set_tls_require
    - e2e_replica_set_tls_require_upgrade
    - e2e_replica_set_tls_require_and_disable
  teardown_task:
    - func: "upload_e2e_logs"

- name: e2e_tls_custom_ca_task_group
  setup_group:
    - func: clone
    - func: download_kubectl
    - func: setup_kubernetes_environment

  tasks:
    - e2e_replica_set_tls_require_custom_ca
    - e2e_sharded_cluster_tls_require_custom_ca
    - e2e_tls_x509_sc_custom_ca
  teardown_task:
    - func: "upload_e2e_logs"

- name: e2e_core_others_task_group
  max_hosts: 1
  setup_group:
    - func: clone
    - func: download_kubectl
    - func: setup_kubernetes_environment

  tasks:
    - e2e_sharded_cluster_scale_shards
  teardown_task:
    - func: "upload_e2e_logs"

- name: e2e_x509_task_group
  max_hosts: 1
  setup_group:
    - func: clone
    - func: download_kubectl
    - func: setup_kubernetes_environment
  tasks:
    - e2e_tls_x509_rs
    - e2e_tls_x509_sc
    - e2e_tls_x509_user_connectivity
    - e2e_tls_x509_users_addition_removal
    - e2e_tls_x509_configure_all_options_rs
    - e2e_tls_x509_configure_all_options_sc
    - e2e_users_schema_validation
  teardown_task:
    - func: "upload_e2e_logs"

- name: e2e_multiple_namespace_task_group
  max_hosts: 100
  setup_group:
    - func: clone
    - func: download_kubectl
    - func: setup_kubernetes_environment
  tasks:
    - e2e_replica_set_different_namespaces
  teardown_task:
    - func: "upload_e2e_logs"

#
# -BUILDVARIANTS-
#
buildvariants:
- name: release_operator
  display_name: "release_operator"
  run_on:
    - archlinux-test
  tasks:
    - ".release_operator"

- name: release_operator_rhel
  display_name: "release_operator_rhel"
  run_on:
    - archlinux-test
  tasks:
    - ".release_operator_rhel"
    - ".build_operator_rhel"

- name: e2e_kube_vanilla_v1.11_om_40_first
  display_name: "e2e_kube_vanilla_v1.11_om_40_first"
  depends_on:
    - name: build_images
      variant: init_test_run
    - name: prepare_cluster_vanilla_1_11
      variant: init_test_run
  run_on:
  - archlinux-test
  stepback: false
  expansions:
    <<: *ops_manager_40_first
    kube_environment_name: vanilla_1_11
  tasks:
  - name: "e2e_core_task_group"
  - name: "e2e_tls_task_group"

- name: e2e_kube_vanilla_v1.11_om_40_current
  display_name: "e2e_kube_vanilla_v1.11_om_40_current"
  depends_on:
    - name: build_images
      variant: init_test_run
    - name: prepare_cluster_vanilla_1_11
      variant: init_test_run
  run_on:
  - archlinux-test
  stepback: false
  expansions:
    <<: *ops_manager_42_current
    kube_environment_name: vanilla_1_11
  tasks:
  - name: "e2e_tls_task_group"
  - name: "e2e_x509_task_group"
  - name: "e2e_core_others_task_group"

- name: e2e_openshift_origin_v3.11_om_40_current
  display_name: "e2e_openshift_origin_v3.11_om_40_current"
  depends_on:
    - name: build_images
      variant: init_test_run
    - name: prepare_cluster_openshift_3_11
      variant: init_test_run
  run_on:
  - archlinux-test
  stepback: false
  expansions:
    <<: *ops_manager_42_current
    kube_environment_name: openshift_3_11
  tasks:
  - name: "e2e_core_task_group"
  - name: "e2e_tls_task_group"
  - name: "e2e_x509_task_group"
  - name: "e2e_core_others_task_group"
  - name: e2e_tls_custom_ca_task_group

- name: e2e_openshift_cloud_qa
  display_name: "e2e_openshift_cloud_qa"
  depends_on:
    - name: build_images
      variant: init_test_run
  run_on:
  - archlinux-test
  stepback: false
  expansions:
    <<: *cloud_manager_qa
    kube_environment_name: openshift_3_11
  tasks:
  - name: "e2e_core_task_group"
  - name: "e2e_tls_task_group"
  - name: "e2e_x509_task_group"
  - name: e2e_tls_custom_ca_task_group

# any subsequent tasks being executed in this build variant should depend upon the previously
# executed task in order to prevent interference
- name: e2e_kube_vanilla_cloud_qa_clusterwide
  display_name: "e2e_kube_vanilla_cloud_qa_clusterwide"
  depends_on:
    - name: build_images
      variant: init_test_run
  run_on:
    - archlinux-test
  stepback: false
  expansions:
    <<: *cloud_manager_qa
    static_namespace: "multiple-namespace-test"
    cluster_name: "e2e.multinamespace.mongokubernetes.com"
    kube_environment_name: vanilla_1_11
  tasks:
    - name: "e2e_multiple_namespace_task_group"

- name: init_test_run
  display_name: init_test_run
  run_on:
  - archlinux-test
  stepback: false
  tasks:
  - name: build_images
  - name: prepare_cluster_openshift_3_11
  - name: prepare_cluster_vanilla_1_11

#
# TODO(rodrigo): There's no mechanism that I know of to avoid running this
#                build variant at master-merge time. Disabling it for now.
#                This will be improved as part of CLOUDP-38550
#
# - name: openshift_rhel_prebuilt_images
#   display_name: openshift_rhel_prebuilt_images
#   depends_on:
#   - name: build_images
#     variant: init_test_run
#   run_on:
#     - archlinux-test
#   expansions:
#     <<: *rhel_prebuilt_images
#   tasks:
#     - name: e2e_core_task_group
#     - name: e2e_tls_task_group
#     - name: e2e_x509_task_group

- name: go_unit_tests
  display_name: "go_unit_tests"
  run_on:
  - archlinux-test
  stepback: false
  tasks:
    - name: "unit_task_group"
