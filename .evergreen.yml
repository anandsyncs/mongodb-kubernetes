ignore:
  - "*.md"
stepback: true
exec_timeout_secs: 1800 # 30 minutes is the longest we'll ever run
variables:
  - &ops_manager_40_first
    ops_manager_version: "4.0.5.50245.20181031T0042Z-1_test"
    ops_manager_namespace: "operator-testing-40-first"
    node_port: 30041
  - &ops_manager_40_current
    ops_manager_version: "4.0.11.50485.20190502T1847Z-1"
    ops_manager_namespace: "operator-testing-40-current"
    node_port: 30040
# TODO: CLOUDP-42158 there are some incompatibilities with the operator and 4.1.x Ops Managers
#  - &ops_manager_41_current
#    ops_manager_version: "4.1.6.54491.20190508T1510Z-1_test"
#    ops_manager_namespace: "operator-testing-41-current"
#    node_port: 30042

  - &cloud_manager_qa
    ops_manager_version: "cloud_qa"


functions:
  "golint":
    - command : shell.exec
      params:
        working_dir: src/github.com/10gen/ops-manager-kubernetes
        script: |
          WORKDIR=${workdir} ./scripts/evergreen/lint_code.sh
  "clone":
    - command: shell.exec
      params:
        script: |
          mkdir -p src/github.com/10gen
    - command: git.get_project
      type: system
      params:
        directory: src/github.com/10gen/ops-manager-kubernetes

  "test_operator":
    - command: shell.exec
      type: system
      params:
        working_dir: src/github.com/10gen/ops-manager-kubernetes
        script: |
          export WORKDIR=${workdir}
          CONTINUE=true ./scripts/evergreen/build_operator

  "build_operator":
    - command: shell.exec
      type: system
      params:
        working_dir: src/github.com/10gen/ops-manager-kubernetes
        script: |
          export WORKDIR=${workdir}
          SKIP_TESTING=true ./scripts/evergreen/build_operator
    - command: s3.put
      params:
        aws_key: ${mms_build_s3_aws_access_key}
        aws_secret: ${mms_build_s3_aws_secret}
        local_file: src/github.com/10gen/ops-manager-kubernetes/docker/mongodb-enterprise-operator/content/mongodb-enterprise-operator
        remote_file: ops-manager-operator/${revision}/mongodb-enterprise-operator
        bucket: ops-manager-kubernetes-build
        permissions: public-read
        content_type: application/octet-stream
  "upload_e2e_artifacts":
    # todo note, that the bucket is public so far - there was something with permissions that didn't allow uploads for the eng test acccount
    - command: s3.put
      params:
        aws_key: ${mms_eng_test_aws_access_key}
        aws_secret: ${mms_eng_test_aws_secret}
        local_files_include_filter:
          - src/github.com/10gen/ops-manager-kubernetes/logs/*
        remote_file: logs/${task_id}/
        bucket: operator-e2e-artifacts
        permissions: public-read
        content_type: application/octet-stream

  # push the latest build of the operator to s3 for the RH build-service to be able to build the RHEL image.
  "push_operator_binary_for_rhel":
    - command: s3.put
      params:
        aws_key: ${mms_build_s3_aws_access_key}
        aws_secret: ${mms_build_s3_aws_secret}
        local_file: src/github.com/10gen/ops-manager-kubernetes/docker/mongodb-enterprise-operator/content/mongodb-enterprise-operator
        remote_file: ops-manager-operator/latest/mongodb-enterprise-operator
        bucket: ops-manager-kubernetes-build
        permissions: public-read
        content_type: application/octet-stream

  "build_rhel_images":
    - command: shell.exec
      params:
        working_dir: src/github.com/10gen/ops-manager-kubernetes
        script: |
          export PATH=${workdir}/bin:$PATH

          if [ "${is_patch}" != "true" ]; then
            echo "Releasing is done only from patches"
            exit 0
          fi

          set -euo

          release="$(jq --raw-output .mongodbOperator < release.json)"

          echo "Performing release $release!"
          scripts/evergreen/build_operator_rhel.sh $release ${rhc_operator_pid}
          scripts/evergreen/build_operator_rhel.sh $release ${rhc_database_pid}

  "setup_jq":
    - command : shell.exec
      params:
        shell: bash
        script: |
          echo "Downloading jq"
          curl -L https://github.com/stedolan/jq/releases/download/jq-1.6/jq-linux64 -o jq
          chmod +x jq
          mkdir -p ${workdir}/bin/
          mv jq ${workdir}/bin/
          echo "Installed jq to ${workdir}/bin/"

  "setup_aws":
    - command: shell.exec
      params:
        shell: bash
        script: |
            src/github.com/10gen/ops-manager-kubernetes/scripts/evergreen/setup_aws

  "setup_docker":
    - command: shell.exec
      params:
        shell: bash
        script: |
            src/github.com/10gen/ops-manager-kubernetes/scripts/evergreen/setup_docker

  "setup_kubectl":
    - command: shell.exec
      params:
        shell: bash
        script: |
          export WORKDIR=${workdir}
          export BINDIR=$WORKDIR/bin ; mkdir -p $BINDIR
          export PATH=$BINDIR:$PATH

          # AWS creds
          export AWS_ACCESS_KEY_ID=${mms_eng_test_aws_access_key}
          export AWS_SECRET_ACCESS_KEY=${mms_eng_test_aws_secret}
          export AWS_REGION=${mms_eng_test_aws_region}
          export AWS_DEFAULT_REGION=${mms_eng_test_aws_region}
          export KOPS_STATE_STORE=s3://kube-om-state-store
          export CLUSTER=e2e.mongokubernetes.com

          echo "Downloading kops"
          curl -s -L https://github.com/kubernetes/kops/releases/download/1.11.1/kops-linux-amd64 -o kops
          chmod +x kops
          mv kops $BINDIR
          kops export kubecfg $CLUSTER

          echo "Downloading kubectl"
          curl -s -LO https://storage.googleapis.com/kubernetes-release/release/v1.11.3/bin/linux/amd64/kubectl
          chmod +x kubectl
          mv kubectl $BINDIR

          echo "Downloading helm"
          HELM=helm.tgz
          curl -s https://storage.googleapis.com/kubernetes-helm/helm-v2.10.0-linux-amd64.tar.gz --output $HELM
          tar xfz $HELM &> /dev/null
          mv linux-amd64/helm $BINDIR
          rm $HELM

          if echo "${build_variant}" | grep -q "openshift"; then
            echo "Downloading OC"
            OC_PKG=oc-linux.tar.gz
            curl -s -L https://github.com/openshift/origin/releases/download/v3.11.0/openshift-origin-client-tools-v3.11.0-0cbc58b-linux-64bit.tar.gz --output $OC_PKG
            tar xfz $OC_PKG &> /dev/null
            mv openshift-origin-client-tools-v3.11.0-0cbc58b-linux-64bit/oc $BINDIR

            echo "Setting up OpenShift variant"
            echo "Token: ${openshift_cluster_token}"
            oc login ${openshift_cluster_url} --token=${openshift_cluster_token} --insecure-skip-tls-verify
            kubectl config use-context default/master-openshift-cluster-mongokubernetes-com:8443/admin
          fi


  "gotest_parse_files":
    - command: gotest.parse_files
      params:
        files: ["*.suite", "src/**/*.suite"]

  "check_evergreen_health":
    - command: shell.exec
      params:
        working_dir: src/github.com/10gen/ops-manager-kubernetes
        script: |
          echo "Checking Evergreen Host Health"
          REVISION=${revision}
          echo "REVISION is $REVISION"

          echo "IS_PATCH: ${is_patch}"
          echo "AUTHOR: ${author}"
          echo "TASK_ID: ${task_id}"
          echo "TASK_NAME: ${task_name}"
          echo "EXECUTION: ${execution}"
          echo "BUILD_ID: ${build_id}"
          echo "BUILD_VARIANT: ${build_variant}"
          echo "VERSION_ID: ${version_id}"
          echo "WORKDIR: ${workdir}"
          echo "REVISION: ${revision}"
          echo "PROJECT: ${project}"
          echo "BRANCH_NAME: ${branch_name}"
          echo "DISTRO_ID: ${distro_id}"
          echo "CREATED_AT: ${created_at}"
          echo "REVISION_ORDER_ID: ${revision_order_id}"
          echo "GITHUB_PR_NUMBER: ${github_pr_number}"
          echo "GITHUB_ORG: ${github_org}"
          echo "GITHUB_REPO: ${github_repo}"
          echo "GITHUB_AUTHOR: ${github_author}"
          echo "HOME: ${HOME}"


          MAX_PERCENT_USED=80
          ROOT_DISK_USED=$(df / | tail -1 | awk ' { print $5 } ' | cut -d"%" -f 1)
          if [ "$ROOT_DISK_USED" -gt "$MAX_PERCENT_USED" ]; then
              printf "More than %s%% used in root partition. Currently used disk %s%%\\n" $MAX_PERCENT_USED $ROOT_DISK_USED
              df /
              exit 1
          fi

          MCI_DIR=/data/mci
          if [ -d $MCI_DIR ]; then
              DATA_DISK_USED=$(df $MCI_DIR | tail -1 | awk ' { print $5 } ' | cut -d"%" -f 1)
              if [ "$DATA_DISK_USED" -gt "$MAX_PERCENT_USED" ]; then
                  printf "Less than %s%% in %s. Currently used disk %s%%\\n" $MAX_PERCENT_USED $MCI_DIR $DATA_DISK_USED
                  df $MCI_DIR
                  exit 1
              fi
          else
              echo "$MCI_DIR does not exists"
              exit
          fi

  "build_and_push_image":
    - command: shell.exec
      params:
        working_dir: src/github.com/10gen/ops-manager-kubernetes
        script: |
          set +x
          # Releases are performed only for patches
          if [ -n "${release_object}" ] && [ "${is_patch}" != "true" ]; then
            echo "Releasing is done only from patches"
            exit 0
          fi

          # AWS creds
          export AWS_ACCESS_KEY_ID=${mms_eng_test_aws_access_key}
          export AWS_SECRET_ACCESS_KEY=${mms_eng_test_aws_secret}

          # Quay prod creds
          export QUAY_PROD_USER=${quay_prod_username}
          export QUAY_PROD_PASSWORD=${quay_prod_robot_token}

          if [ "${tag_images_with_version}" = "true" ]; then
            export REVISION=${version_id}
          fi

          python3 -m venv venv
          source venv/bin/activate
          python3 -m pip install -r scripts/evergreen/requirements.txt

          # Always consider the caveat of "${var} vs $var" in evergreen!!
          docker_args="${docker_args}"

          # Releases override REVISION from predefined tags
          if [ -n "${release_object}" ]; then
            export REVISION=$(./scripts/evergreen/read_release_version.py --release-app ${release_object})

            # When releasing mongodb enterprise the docker tag and version of MongoDB are the same
            if [ "${release_object}" = "mongodbEnterprise" ]; then
              export docker_args="MONGO_VERSION=$REVISION"
            fi
          fi

          ./scripts/evergreen/build_and_push.py \
              --image ${image_name} \
              --tag "$REVISION" \
              --registry "${env}" \
              --path "${path}" \
              --docker-args "$docker_args"

  # Populate the Ops Manager Kubernetes Perpetual instance, env vars
  "omkp_setup": &omkp_setup
    command: shell.exec
    params:
      working_dir: src/github.com/10gen/ops-manager-kubernetes
      shell: bash
      script: |
        if [[ "${omkp_enabled}" == "true" ]]; then
          echo "echo Using the Ops Manager Kubernetes Perpetual instance..."  > ${workdir}/.ops-manager-env
          echo "export OM_BASE_URL=${omkp_host}"                             >> ${workdir}/.ops-manager-env
          echo "export OM_USER=${omkp_user}"                                 >> ${workdir}/.ops-manager-env
          echo "export OM_API_KEY=${omkp_api_key}"                           >> ${workdir}/.ops-manager-env
          echo "export OM_EXTERNALLY_CONFIGURED=${omkp_enabled}"             >> ${workdir}/.ops-manager-env
          echo "Using the Ops Manager Kubernetes Perpetual Instance, env stored at '${workdir}/.ops-manager-env')"
        else
          echo "Skipping the Ops Manager Kubernetes Perpetual config (omkp_enabled=${omkp_enabled})"
        fi

  "configure_cloud_manager_qa": &configure_cloud_manager_qa
    command: shell.exec
    params:
      working_dir: src/github.com/10gen/ops-manager-kubernetes
      shell: bash
      script: |
        echo "Configuring Cloud Manager QA"
        if [ "${build_variant}" = "e2e_openshift_cloud_qa" ]; then
          echo "export OM_BASE_URL=${e2e_cloud_qa_baseurl}"      > ${workdir}/.ops-manager-env
          echo "export OM_USER=${e2e_cloud_qa_user}"            >> ${workdir}/.ops-manager-env
          echo "export OM_API_KEY=${e2e_cloud_qa_apikey}"       >> ${workdir}/.ops-manager-env
          echo "export OM_ORGID=${e2e_cloud_qa_orgid}"          >> ${workdir}/.ops-manager-env
          echo "export OM_EXTERNALLY_CONFIGURED=true"           >> ${workdir}/.ops-manager-env
        fi

  "e2e_test":
    - *omkp_setup
    - *configure_cloud_manager_qa
    - command: shell.exec
      params:
        working_dir: src/github.com/10gen/ops-manager-kubernetes
        script: |
          # revision is only supposed to be set in evergreen
          export WORKDIR=${workdir}
          export IS_EVERGREEN=${revision}
          export REVISION=$(git rev-parse HEAD)
          export PATH=${workdir}/bin:$PATH
          export VERSION_ID=${version_id}
          export MANAGED_SECURITY_CONTEXT="false"
          export BUILD_VARIANT=${build_variant}
          export TASK_ID=${task_id}
          export TASK_NAME=${task_name}
          export OPS_MANAGER_NAMESPACE=${ops_manager_namespace}
          export NODE_PORT=${node_port}

          export MMS_VERSION=${ops_manager_version}

          echo "Build Variant: ${build_variant}"
          set -e # fail on first failed test

          if echo "${build_variant}" | grep -q "openshift"; then
            echo "Setting up OpenShift variant"
            export MANAGED_SECURITY_CONTEXT="true"
          fi

          # Externally Configured Ops Manager (Perpetual Instance or Cloud Manager)
          test -f ${workdir}/.ops-manager-env && source ${workdir}/.ops-manager-env

          # A complete environment is set each time.
          WATCH_NAMESPACE=${watch_namespace} TASK_NAME=${task_name} ./scripts/evergreen/e2e_tests.sh

  "prepare_test_env":
    - *omkp_setup
    - *configure_cloud_manager_qa
    - command: shell.exec
      params:
        working_dir: src/github.com/10gen/ops-manager-kubernetes/scripts/evergreen
        script: |
          export PATH=${workdir}/bin:$PATH
          export AWS_ACCESS_KEY_ID=${mms_eng_test_aws_access_key}
          export AWS_SECRET_ACCESS_KEY=${mms_eng_test_aws_secret}
          export AWS_REGION=${mms_eng_test_aws_region}
          export AWS_DEFAULT_REGION=${mms_eng_test_aws_region}

          # Externally managed Ops/Cloud Manager instance get configured here
          test -f ${workdir}/.ops-manager-env && source ${workdir}/.ops-manager-env

          echo "Preparing Ops/Cloud Manager Version: ${ops_manager_version} in namespace ${ops_manager_namespace} on node port ${node_port}"
          ./prepare_test_env ${ops_manager_namespace} ${ops_manager_version} ${node_port}

  "post_task":
  - command: shell.exec
    params:
      script: |
        if [ ! "$(which docker)" ]; then
          # if not docker present, no images either.
          exit 0
        fi

        echo "Removing all docker images in this machine"
        docker rmi -f $(docker images -a -q) &> /dev/null || true

        echo "Are there any remaining Docker images in this host?"
        docker images

post:
   - func: "post_task"

tasks:
- name: unit_tests
  tags: ["unit_tests"]
  commands:
    - func: "test_operator"
    - func: "gotest_parse_files"

- name: unit_tests_lint
  tags: ["unit_tests_lint"]
  commands:
    - func: "golint"

- name: release_operator
  tags: ["release_operator"]
  patch_only: true
  commands:
    - func: "clone"
    - func: "build_operator"
    - func: "setup_docker"
    - func: "build_and_push_image"
      vars:
        release_object: "mongodbOperator"
        env: production
        image_name: mongodb-enterprise-operator
    - func: "build_and_push_image"
      vars:
        release_object: "mongodbOperator"
        env: production
        image_name: mongodb-enterprise-database

- name: release_operator_rhel_connect
  tags: ["release_operator_rhel"]
  patch_only: true
  commands:
    - func: "clone"
    - func: "setup_jq"
    - func: "build_operator"
    - func: "push_operator_binary_for_rhel"
    - func: "build_rhel_images"

# Note that this task doesn't work so far as it needs different quay account and token
- name: release_mongodb_enterprise
  tags: ["release_mongodb_enterprise"]
  patch_only: true
  commands:
    - func: "clone"
    - func: "setup_docker"
    - func: "build_and_push_image"
      vars:
        release_object: "mongodbEnterprise"
        env: production
        image_name: mongodb-enterprise
        path: mongodb-enterprise/4.0

- name: build_and_push_images_development
  tags: ["build_and_push_images_development"]
  commands:
    - func: "clone"
    - func: "build_operator"
    - func: "setup_docker"
    - func: "setup_aws"
    - func: "build_and_push_image"
      vars:
        env: development
        image_name: mongodb-enterprise-operator
        tag_images_with_version: "true"
    - func: "build_and_push_image"
      vars:
        env: development
        image_name: mongodb-enterprise-database
        tag_images_with_version: "true"

- name: setup_e2e
  # 40 minutes max (sometimes cleanup procedure may take very long because of slow PVCs removal...)
  exec_timeout_secs: 2400
  commands:
    - func: "clone"
    - func: "check_evergreen_health"
    - func: "setup_aws"
    - func: "setup_jq"
    - func: "setup_docker"
    - func: "build_operator"
    - func: "build_and_push_image"
      vars:
        env: development
        image_name: mongodb-enterprise-operator
        tag_images_with_version: "true"
    - func: "build_and_push_image"
      vars:
        env: development
        image_name: mongodb-enterprise-database
        tag_images_with_version: "true"
    - func: "build_and_push_image"
      vars:
        env: development
        image_name: mongodb-enterprise-tests
        tag_images_with_version: "true"
    - func: "setup_kubectl"
    - func: "prepare_test_env"

- name: e2e_standalone_schema_validation
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_replica_set_schema_validation
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_sharded_cluster_schema_validation
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_users_schema_validation
  exec_timeout_secs: 600 # 10 minutes max
  commands:
    - func: "e2e_test"

# Removing this test as it creates a cluster wide operator
# that collides with every other operator running in the
# cluster, breaking havoc
# - name: e2e_replica_set_different_namespaces
#   exec_timeout_secs: 600 # 10 minutes max
#   commands:
#     - func: "e2e_test"
#       vars:
#         test_name: replica_set_different_namespaces
#         watch_namespace: "*"

- name: e2e_standalone_config_map
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_standalone_groups
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_standalone_recovery
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_standalone_recovery_k8s
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_replica_set
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

# - name: e2e_replica_set
#   exec_timeout_secs: 600 # 10 minutes max
#   commands:
#     - func: "e2e_test"
#       vars:
#         test_name: replica_set

- name: e2e_replica_set_recovery
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_replica_set_config_map
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_replica_set_ent
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_replica_set_groups
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_replica_set_pv
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_replica_set_pv_multiple
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_replica_set_8_members
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_sharded_cluster
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_sharded_cluster_pv
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_sharded_cluster_recovery
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_sharded_cluster_secret
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_standalone_type_change_recovery
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_all_mongodb_resources_parallel
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_standalone_upgrade_downgrade
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_replica_set_upgrade_downgrade
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_sharded_cluster_upgrade_downgrade
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_standalone_no_tls_no_status_is_set
  exec_timeout_secs: 600
  commands:
  - func: "e2e_test"

- name: e2e_replica_set_tls_allow
  exec_timeout_secs: 600
  commands:
  - func: "e2e_test"

- name: e2e_replica_set_tls_prefer
  exec_timeout_secs: 600
  commands:
  - func: "e2e_test"

- name: e2e_replica_set_tls_require
  exec_timeout_secs: 600
  commands:
  - func: "e2e_test"

- name: e2e_replica_set_tls_require_and_disable
  exec_timeout_secs: 600
  commands:
  - func: "e2e_test"

- name: e2e_tls_multiple_different_ssl_configs
  exec_timeout_secs: 600
  commands:
  - func: "e2e_test"

- name: e2e_replica_set_tls_require_upgrade
  exec_timeout_secs: 600
  commands:
  - func: "e2e_test"

- name: e2e_sharded_cluster_tls_require
  exec_timeout_secs: 600
  commands:
  - func: "e2e_test"

- name: e2e_tls_x509_rs
  # longer timeout than usual as this test tests recovery from bad states which can take some time
  exec_timeout_secs: 900
  commands:
  - func: "e2e_test"

- name: e2e_tls_x509_sc
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_tls_x509_user_connectivity
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

task_groups:
- name: unit_task_group
  setup_group:
    - func: "clone"
  tasks:
    - unit_tests
    - unit_tests_lint

- name: e2e_core_task_group
  max_hosts: 100
  setup_group:
    - func: "clone"
    - func: "setup_kubectl"
  tasks:
    # Weird, but if the name tasks without "e2e_" (so equal to 'test_name' param) - evergreen doesn't run them...
    - e2e_all_mongodb_resources_parallel
    - e2e_standalone_config_map
    - e2e_standalone_groups
    - e2e_standalone_schema_validation
    - e2e_standalone_type_change_recovery
    - e2e_standalone_upgrade_downgrade
    - e2e_sharded_cluster_schema_validation
    - e2e_standalone_recovery
    - e2e_standalone_recovery_k8s
    - e2e_replica_set
    - e2e_replica_set_config_map
    - e2e_replica_set_ent
    - e2e_replica_set_groups
    - e2e_replica_set_recovery
    - e2e_replica_set_pv
    - e2e_replica_set_pv_multiple
    - e2e_replica_set_schema_validation
    - e2e_replica_set_upgrade_downgrade
    - e2e_replica_set_8_members
    # - e2e_replica_set_different_namespaces
    - e2e_sharded_cluster
    - e2e_sharded_cluster_pv
    - e2e_sharded_cluster_recovery
    - e2e_sharded_cluster_secret
    - e2e_sharded_cluster_upgrade_downgrade
  teardown_task:
    - func: "upload_e2e_artifacts"

- name: e2e_tls_task_group
  max_hosts: 100
  setup_group:
    - func: "clone"
    - func: "setup_kubectl"
  tasks:
    - e2e_sharded_cluster_tls_require
    - e2e_standalone_no_tls_no_status_is_set
    - e2e_replica_set_tls_allow
    - e2e_replica_set_tls_prefer
    - e2e_replica_set_tls_require
    - e2e_replica_set_tls_require_upgrade
    - e2e_replica_set_tls_require_and_disable
  teardown_task:
    - func: "upload_e2e_artifacts"

- name: e2e_x509_task_group
  max_hosts: 100
  setup_group:
    - func: "clone"
    - func: "setup_kubectl"
  tasks:
    - e2e_tls_x509_rs
    - e2e_tls_x509_sc
    - e2e_tls_x509_user_connectivity
    - e2e_users_schema_validation
  teardown_task:
    - func: "upload_e2e_artifacts"

buildvariants:

- name: release_operator
  display_name: "release_operator"
  run_on:
    - archlinux-test
  tasks:
    - ".release_operator"

- name: release_operator_rhel
  display_name: "release_operator_rhel"
  run_on:
    - archlinux-test
  tasks:
    - ".release_operator_rhel"

- name: build_and_push_images_development
  display_name: "build_and_push_images_development"
  run_on:
    - archlinux-test
  stepback: false
  tasks:
    - ".build_and_push_images_development"

- name: e2e_kube_vanilla_v1.11_om_40_first
  display_name: "e2e_kube_vanilla_v1.11_om_40_first"
  run_on:
  - archlinux-test
  stepback: false
  expansions:
    <<: *ops_manager_40_first
  tasks:
  - name: "setup_e2e"
  - name: "e2e_core_task_group"
    depends_on:
    - name: setup_e2e
  - name: "e2e_tls_task_group"
    depends_on:
    - name: setup_e2e

- name: e2e_kube_vanilla_v1.11_om_40_current
  display_name: "e2e_kube_vanilla_v1.11_om_40_current"
  run_on:
  - archlinux-test
  stepback: false
  expansions:
    <<: *ops_manager_40_current
  tasks:
  - name: "setup_e2e"
  - name: "e2e_tls_task_group"
    depends_on:
      - name: setup_e2e
  - name: "e2e_x509_task_group"
    depends_on:
      - name: setup_e2e

- name: e2e_openshift_origin_v3.11_om_40_current
  display_name: "e2e_openshift_origin_v3.11_om_40_current"
  run_on:
  - archlinux-test
  stepback: false
  expansions:
    <<: *ops_manager_40_current
  tasks:
  - name: "setup_e2e"
  - name: "e2e_core_task_group"
    depends_on:
    - name: setup_e2e
  - name: "e2e_tls_task_group"
    depends_on:
    - name: setup_e2e
  - name: "e2e_x509_task_group"
    depends_on:
      - name: setup_e2e

- name: e2e_openshift_cloud_qa
  display_name: "e2e_openshift_cloud_qa"
  run_on:
  - archlinux-test
  stepback: false
  expansions:
    <<: *cloud_manager_qa
  tasks:
  - name: "setup_e2e"
  - name: "e2e_core_task_group"
    depends_on:
    - name: setup_e2e
  - name: "e2e_tls_task_group"
    depends_on:
    - name: setup_e2e
  - name: "e2e_x509_task_group"
    depends_on:
      - name: setup_e2e

- name: go_unit_tests
  display_name: "go_unit_tests"
  run_on:
  - archlinux-test
  stepback: false
  tasks:
    - name: "unit_task_group"
