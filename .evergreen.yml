ignore:
  - "*.md"
  - "public/support/*"
  - "public/samples/*"
stepback: true
exec_timeout_secs: 1800 # 30 minutes is the longest we'll ever run
variables:
  - &ops_manager_40_first
    ops_manager_version: "4.0.11.50485.20190502T1847Z-1_test"
    ops_manager_namespace: "operator-testing-40-first"
    node_port: 30041
  - &ops_manager_42_current
    ops_manager_version: "4.2.0.56506.20190804T0058Z-1_test"
    ops_manager_namespace: "operator-testing-42-current"
    node_port: 30043

  - &cloud_manager_qa
    ops_manager_version: "cloud_qa"

  # These need to be defined manually.
  # TODO(rodrigo): This will be improved as part of CLOUDP-38550.
  - &rhel_prebuilt_images
    prebuilt_tag: latest
    prebuilt_database_name: mdb-database
    prebuilt_operator_name: mdb-operator
    prebuilt_registry: quay.io/some
    prebuilt_images_run: true

    # Openshift v3.11 Testing Environment
  - &kubernetes_environment_openshift_3_11:
    kube_environment_name: openshift_3_11

    # Kops Vanilla Kubernetes v1.15
  - &kubernetes_environment_vanilla_1_15:
    kube_environment_name: vanilla_1_15

functions:
  "golint":
    - command: shell.exec
      params:
        working_dir: src/github.com/10gen/ops-manager-kubernetes
        script: |
          WORKDIR=${workdir} ./scripts/evergreen/lint_code.sh
  "clone":
    - command: shell.exec
      params:
        script: |
          mkdir -p src/github.com/10gen
    - command: git.get_project
      type: system
      params:
        directory: src/github.com/10gen/ops-manager-kubernetes

  "test_operator":
    - command: shell.exec
      type: system
      params:
        working_dir: src/github.com/10gen/ops-manager-kubernetes
        script: |
          export WORKDIR=${workdir}
          CONTINUE=true ./scripts/evergreen/build_operator

  "build_operator":
    - command: shell.exec
      type: system
      params:
        shell: bash
        working_dir: src/github.com/10gen/ops-manager-kubernetes
        script: |
          export WORKDIR=${workdir}

          if [[ "${task_name}" = "release_operator_rhel_connect" ]] || [[ "${task_name}" = "release_operator" ]]; then
            # Make sure release matches a correct release name.

            if ! git status --short; then
              echo "The repository is dirty. Make sure no uncommited or modified files were added to the patch."
              git status --short
              exit 1
            fi

            release_version="$(git describe --dirty)"
            if ! [[ "$release_version" =~ ^[0-9]{1,2}\.[0-9]{1,2}\.[0-9]{1,2}$ ]]; then
              echo "$release_version does not conform to what we expect as a version label."
              exit 1
            fi

            echo "Proceeding with release $release_version"
          fi

          ( cd docker/mongodb-enterprise-operator && ../Dockerfile.gen ${distro} > Dockerfile )

          SKIP_TESTING=true ./scripts/evergreen/build_operator

  build_database:
    - command: shell.exec
      type: system
      params:
        shell: bash
        working_dir: src/github.com/10gen/ops-manager-kubernetes
        script: |
          export GOPATH="${workdir}"
          export GOROOT="/usr/lib/go"
          export GOBIN="/opt/golang/go1.13"
          export GOFLAGS="-mod=vendor"
          export PATH="$GOBIN:$PATH"

          ( cd docker/mongodb-enterprise-database && ../Dockerfile.gen ${distro} > Dockerfile )

          cd probe
          go build -i -o readinessprobe

          mv readinessprobe ../docker/mongodb-enterprise-database/content/readinessprobe

    - command: s3.put
      params:
        aws_key: ${mms_build_s3_aws_access_key}
        aws_secret: ${mms_build_s3_aws_secret}
        local_file: src/github.com/10gen/ops-manager-kubernetes/docker/mongodb-enterprise-operator/content/mongodb-enterprise-operator
        remote_file: ops-manager-operator/${revision}/mongodb-enterprise-operator
        bucket: ops-manager-kubernetes-build
        permissions: public-read
        content_type: application/octet-stream

  "upload_e2e_logs":
    # todo note, that the bucket is public so far - there was something with permissions that didn't allow uploads for the eng test acccount
    - command: s3.put
      params:
        aws_key: ${mms_eng_test_aws_access_key}
        aws_secret: ${mms_eng_test_aws_secret}
        local_files_include_filter:
          - src/github.com/10gen/ops-manager-kubernetes/logs/*
        remote_file: logs/${task_id}/
        bucket: operator-e2e-artifacts
        permissions: public-read
        content_type: text/plain

  # push the latest build of the operator to s3 for the RH build-service to be able to build the RHEL image.
  push_operator_binary_for_rhel:
    - command: s3.put
      params:
        aws_key: ${mms_build_s3_aws_access_key}
        aws_secret: ${mms_build_s3_aws_secret}
        local_file: src/github.com/10gen/ops-manager-kubernetes/docker/mongodb-enterprise-operator/content/mongodb-enterprise-operator
        remote_file: ops-manager-operator/latest/mongodb-enterprise-operator
        bucket: ops-manager-kubernetes-build
        permissions: public-read
        content_type: application/octet-stream

  push_database_readiness_probe_for_rhel:
    - command: s3.put
      params:
        aws_key: ${mms_build_s3_aws_access_key}
        aws_secret: ${mms_build_s3_aws_secret}
        local_file: src/github.com/10gen/ops-manager-kubernetes/docker/mongodb-enterprise-database/content/readinessprobe
        remote_file: ops-manager-operator/latest/database-readiness-probe
        bucket: ops-manager-kubernetes-build
        permissions: public-read
        content_type: application/octet-stream

  "build_rhel_images":
    - command: shell.exec
      params:
        working_dir: src/github.com/10gen/ops-manager-kubernetes
        script: |
          export PATH=${workdir}/bin:$PATH

          if [ "${is_patch}" != "true" ]; then
            echo "Releasing is done only from patches"
            exit 0
          fi

          set -euo

          release="$(jq --raw-output .mongodbOperator < release.json)"

          echo "Performing release $release!"
          scripts/evergreen/build_operator_rhel.sh $release ${rhc_operator_pid}
          scripts/evergreen/build_operator_rhel.sh $release ${rhc_database_pid}

  "setup_jq":
    - command: shell.exec
      params:
        shell: bash
        script: |
          echo "Downloading jq"
          curl -L https://github.com/stedolan/jq/releases/download/jq-1.6/jq-linux64 -o jq
          chmod +x jq
          mkdir -p ${workdir}/bin/
          mv jq ${workdir}/bin/
          echo "Installed jq to ${workdir}/bin/"

  "setup_aws":
    - command: shell.exec
      params:
        shell: bash
        script: |
            src/github.com/10gen/ops-manager-kubernetes/scripts/evergreen/setup_aws

  "setup_docker":
    - command: shell.exec
      params:
        shell: bash
        script: |
            src/github.com/10gen/ops-manager-kubernetes/scripts/evergreen/setup_docker

  download_kubectl:
    - command: shell.exec
      params:
        shell: bash
        script: |
          export WORKDIR=${workdir}
          export BINDIR=$WORKDIR/bin ; mkdir -p $BINDIR
          export PATH=$BINDIR:$PATH

          echo "Downloading kubectl"
          curl -s -LO https://storage.googleapis.com/kubernetes-release/release/v1.15.4/bin/linux/amd64/kubectl
          chmod +x kubectl
          mv kubectl $BINDIR

          echo "Downloading helm"
          HELM=helm.tgz
          curl -s https://storage.googleapis.com/kubernetes-helm/helm-v2.10.0-linux-amd64.tar.gz --output $HELM
          tar xfz $HELM &> /dev/null
          mv linux-amd64/helm $BINDIR
          rm $HELM


  setup_kubernetes_environment:
  - command: shell.exec
    params:
      shell: bash
      script: |
          export WORKDIR=${workdir}
          export BINDIR=$WORKDIR/bin
          export PATH=$BINDIR:$PATH

          if [ "${kube_environment_name}" = "openshift_3_11" ]; then
            echo "Downloading OC & setting up Openshift cluster"
            OC_PKG=oc-linux.tar.gz
            curl -s -L https://github.com/openshift/origin/releases/download/v3.11.0/openshift-origin-client-tools-v3.11.0-0cbc58b-linux-64bit.tar.gz --output $OC_PKG
            tar xfz $OC_PKG &> /dev/null
            mv openshift-origin-client-tools-v3.11.0-0cbc58b-linux-64bit/oc $BINDIR

            echo "Setting up OpenShift variant"
            echo "Token: ${openshift_cluster_token}"
            oc login ${openshift_cluster_url} --token=${openshift_cluster_token} --insecure-skip-tls-verify
            kubectl config use-context default/master-openshift-cluster-mongokubernetes-com:8443/admin
          elif [ "${kube_environment_name}" = "vanilla_1_15" ]; then
            if [ ! -z ${cluster_name} ]; then
              export CLUSTER=${cluster_name}
            else
              export CLUSTER=e2e.mongokubernetes.com
            fi

            # AWS creds
            export AWS_ACCESS_KEY_ID=${mms_eng_test_aws_access_key}
            export AWS_SECRET_ACCESS_KEY=${mms_eng_test_aws_secret}
            export AWS_REGION=${mms_eng_test_aws_region}
            export AWS_DEFAULT_REGION=${mms_eng_test_aws_region}
            export KOPS_STATE_STORE=s3://kube-om-state-store

            echo "Downloading kops"
            curl -s -L https://github.com/kubernetes/kops/releases/download/1.14.0/kops-linux-amd64 -o kops
            chmod +x kops
            mv kops $BINDIR

            if ! kops get clusters | grep -q $CLUSTER; then
              echo "Cluster $CLUSTER not found, exiting..."
              echo run "make recreate-e2e-kops imsure=yes cluster=$CLUSTER"
              kops get clusters
              exit 1
            fi

            kops export kubecfg $CLUSTER

          else
            echo "kube_environment_name not recognized"
            echo "value is <<${kube_environment_name}>>. If empty it means it was not set"
          fi


  "gotest_parse_files":
    - command: gotest.parse_files
      params:
        files: ["*.suite", "src/**/*.suite"]

  "check_evergreen_health":
    - command: shell.exec
      params:
        working_dir: src/github.com/10gen/ops-manager-kubernetes
        script: |
          mkdir -p logs
          cat <<EOF >> logs/evergreen_environment.txt
          Checking Evergreen Host Health
          IS_PATCH=${is_patch}
          AUTHOR=${author}
          TASK_ID=${task_id}
          TASK_NAME=${task_name}
          EXECUTION=${execution}
          BUILD_ID=${build_id}
          BUILD_VARIANT=${build_variant}
          VERSION_ID=${version_id}
          WORKDIR=${workdir}
          REVISION=${revision}
          PROJECT=${project}
          BRANCH_NAME=${branch_name}
          DISTRO_ID=${distro_id}
          CREATED_AT=${created_at}
          REVISION_ORDER_ID=${revision_order_id}
          GITHUB_PR_NUMBER=${github_pr_number}
          GITHUB_ORG=${github_org}
          GITHUB_REPO=${github_repo}
          GITHUB_AUTHOR=${github_author}
          EOF

  # Uploads the Docker build context to S3 in order for Kaniko to be able to build the images.
  upload_e2e_build_context:
  - command: shell.exec
    params:
      working_dir: src/github.com/10gen/ops-manager-kubernetes
      script: |
        tar -C docker/mongodb-enterprise-operator -zcvf operator-context.tar.gz .
        tar -C docker/mongodb-enterprise-database -zcvf database-context.tar.gz .
  - command: s3.put
    params:
      aws_key: ${mms_build_s3_aws_access_key}
      aws_secret: ${mms_build_s3_aws_secret}
      local_files_include_filter:
        - src/github.com/10gen/ops-manager-kubernetes/*-context.tar.gz
      remote_file: ops-manager-operator/${version_id}:${distro}/contexts/
      bucket: ops-manager-kubernetes-build
      permissions: public-read
      content_type: application/octet-stream

  # Builds the Docker images using Kaniko using the context present in S3
  build_e2e_images:
  - command: shell.exec
    params:
      working_dir: src/github.com/10gen/ops-manager-kubernetes
      script: |
        export PATH=${workdir}/bin:$PATH
        export VERSION=${version_id}
        export AWS_ACCESS_KEY_ID=${mms_eng_test_aws_access_key}
        export AWS_SECRET_ACCESS_KEY=${mms_eng_test_aws_secret}
        export AWS_REGION=${mms_eng_test_aws_region}
        export AWS_DEFAULT_REGION=${mms_eng_test_aws_region}

        source scripts/evergreen/build_docker_image.sh
        source scripts/funcs
        short_version=$(split_version_into_sha ${version_id})

        for image_type in "operator" "database"; do
          ensure_ecr_repository "dev/${distro}/mongodb-enterprise-$image_type"
          ./scripts/evergreen/build_docker_image.sh \
              "268558157000.dkr.ecr.us-east-1.amazonaws.com/dev/${distro}/mongodb-enterprise-$image_type" \
              "$VERSION" \
              "s3://ops-manager-kubernetes-build/ops-manager-operator/$VERSION:${distro}/contexts/$image_type-context.tar.gz" \
              "a=1" \
              "268558157000.dkr.ecr.us-east-1.amazonaws.com/cache/mongodb-enterprise-$image_type" \
              "$image_type-build-$short_version-${distro}"
        done
        echo "All images have been scheduled to be built."
        all_finished="false"
        while [[ $all_finished == "false" ]]; do
          sleep 5
          all_finished="true"
          for image_type in "operator" "database"; do
            if [[ $(kubectl -n construction-site get pod --selector "podbuilderid=$image_type-build-$short_version-${distro}" -o jsonpath='{.items[0].status.phase}') != "Succeeded" ]]; then
              status=$(kubectl -n construction-site get pod --selector "podbuilderid=$image_type-build-$short_version-${distro}" -o jsonpath='{.status.phase}')
              echo "Pod with label $image_type-build-$short_version-${distro} has not finished yet. Status is $status"
              all_finished="false"
            else
              echo "Pod with label $image_type-build-$short_version-${distro} has finished"
            fi
          done
        done
        echo "Construction Pods have finished, we are good to start the tests."

  "build_and_push_image":
    - command: shell.exec
      params:
        working_dir: src/github.com/10gen/ops-manager-kubernetes
        script: |
          set +x
          # Releases are performed only for patches
          if [ -n "${release_object}" ] && [ "${is_patch}" != "true" ]; then
            echo "Releasing is done only from patches"
            exit 0
          fi

          # AWS creds
          export AWS_ACCESS_KEY_ID=${mms_eng_test_aws_access_key}
          export AWS_SECRET_ACCESS_KEY=${mms_eng_test_aws_secret}

          # Quay prod creds
          export QUAY_PROD_USER=${quay_prod_username}
          export QUAY_PROD_PASSWORD=${quay_prod_robot_token}

          if [ "${tag_images_with_version}" = "true" ]; then
            export REVISION=${version_id}
          fi

          python3 -m venv venv
          source venv/bin/activate
          python3 -m pip install -r scripts/evergreen/requirements.txt

          # Always consider the caveat of "${var} vs $var" in evergreen!!
          docker_args="${docker_args}"

          # Releases override REVISION from predefined tags
          if [ -n "${release_object}" ]; then
            export REVISION=$(./scripts/evergreen/read_release_version.py --release-app ${release_object})

            # When releasing mongodb enterprise the docker tag and version of MongoDB are the same
            if [ "${release_object}" = "mongodbEnterprise" ]; then
              export docker_args="MONGO_VERSION=$REVISION"
            fi
          fi

          ./scripts/evergreen/build_and_push.py \
              --image ${image_name} \
              --tag "$REVISION" \
              --registry "${env}" \
              --path "${path}" \
              --docker-args "$docker_args" \
              $(if [ -n "${release_object}" ]; then echo '--with-latest-tag'; fi)

  # Populate the Ops Manager Kubernetes Perpetual instance, env vars
  "omkp_setup": &omkp_setup
    command: shell.exec
    params:
      working_dir: src/github.com/10gen/ops-manager-kubernetes
      shell: bash
      script: |
        if [[ "${omkp_enabled}" == "true" ]]; then
          echo "echo Using the Ops Manager Kubernetes Perpetual instance..."  > ${workdir}/.ops-manager-env
          echo "export OM_BASE_URL=${omkp_host}"                             >> ${workdir}/.ops-manager-env
          echo "export OM_USER=${omkp_user}"                                 >> ${workdir}/.ops-manager-env
          echo "export OM_API_KEY=${omkp_api_key}"                           >> ${workdir}/.ops-manager-env
          echo "export OM_EXTERNALLY_CONFIGURED=${omkp_enabled}"             >> ${workdir}/.ops-manager-env
          echo "Using the Ops Manager Kubernetes Perpetual Instance, env stored at '${workdir}/.ops-manager-env')"
        else
          echo "Skipping the Ops Manager Kubernetes Perpetual config (omkp_enabled=${omkp_enabled})"
        fi

  "configure_cloud_manager_qa": &configure_cloud_manager_qa
    command: shell.exec
    params:
      working_dir: src/github.com/10gen/ops-manager-kubernetes
      shell: bash
      script: |
        echo "Configuring Cloud Manager QA"
        if echo "${build_variant}" | grep -Eq "cloud_qa|multiple_namespace|openshift_rhel_prebuilt_images"; then
          echo "export OM_BASE_URL=${e2e_cloud_qa_baseurl}"      > ${workdir}/.ops-manager-env
          echo "export OM_USER=${e2e_cloud_qa_user}"            >> ${workdir}/.ops-manager-env
          echo "export OM_API_KEY=${e2e_cloud_qa_apikey}"       >> ${workdir}/.ops-manager-env
          echo "export OM_ORGID=${e2e_cloud_qa_orgid}"          >> ${workdir}/.ops-manager-env
          echo "export OM_EXTERNALLY_CONFIGURED=true"           >> ${workdir}/.ops-manager-env
        fi

  "e2e_test":
    - *omkp_setup
    - *configure_cloud_manager_qa
    - command: shell.exec
      params:
        working_dir: src/github.com/10gen/ops-manager-kubernetes
        script: |
          # revision is only supposed to be set in evergreen
          if [ "${prebuilt_images_run}" = "true" ]; then
            echo "Testing prebuilt images"
            export REVISION=${prebuilt_tag}
            export DATABASE_NAME=${prebuilt_database_name}
            export OPERATOR_NAME=${prebuilt_operator_name}
            export REGISTRY=${prebuilt_registry}
            export TEST_IMAGE_TAG=${version_id}
          else
            export REVISION="$(git rev-parse HEAD)"
            export VERSION_ID="${version_id}"
            export OPERATOR_NAME=mongodb-enterprise-operator
            export DATABASE_NAME=mongodb-enterprise-database
            export REGISTRY="268558157000.dkr.ecr.us-east-1.amazonaws.com/dev/${distro}"
          fi

          echo "Distro is ${distro}. Should have been set in buildvariant!"

          export WORKDIR=${workdir}
          export IS_EVERGREEN=${revision}
          export PATH=${workdir}/bin:$PATH
          export MANAGED_SECURITY_CONTEXT="false"
          export BUILD_VARIANT=${build_variant}
          export TASK_ID=${task_id}
          export TASK_NAME=${task_name}
          export OPS_MANAGER_NAMESPACE=${ops_manager_namespace}
          export NODE_PORT=${node_port}

          export MMS_VERSION=${ops_manager_version}

          echo "Build Variant: ${build_variant}"
          set -e # fail on first failed test

          if echo "${build_variant}" | grep -q "openshift"; then
            echo "Setting up OpenShift variant"
            export MANAGED_SECURITY_CONTEXT="true"
          fi

          # Externally Configured Ops Manager (Perpetual Instance or Cloud Manager)
          test -f ${workdir}/.ops-manager-env && source ${workdir}/.ops-manager-env

          # A complete environment is set each time.
          WATCH_NAMESPACE=${watch_namespace} TASK_NAME=${task_name} STATIC_NAMESPACE=${static_namespace} \
            TEST_MODE=${test_mode} ./scripts/evergreen/e2e_tests.sh

  "e2e_test_upgrade_operator":
    - *configure_cloud_manager_qa
    - command: shell.exec
      params:
        working_dir: src/github.com/10gen/ops-manager-kubernetes
        script: |
          # revision is only supposed to be set in evergreen
          export WORKDIR=${workdir}
          export IS_EVERGREEN=${revision}
          export PATH=${workdir}/bin:$PATH
          export REVISION=$(git rev-parse HEAD)
          export PATH=${workdir}/bin:$PATH
          export VERSION_ID="${version_id}"
          export MANAGED_SECURITY_CONTEXT="true"
          export BUILD_VARIANT=${build_variant}
          export TASK_ID=${task_id}
          export TASK_NAME=${task_name}
          export OPS_MANAGER_NAMESPACE=${ops_manager_namespace}
          export NODE_PORT=${node_port}
          export OPERATOR_NAME=mongodb-enterprise-operator
          export DATABASE_NAME=mongodb-enterprise-database

          set -e # fail on first failed test

          # Externally Configured Ops Manager (Perpetual Instance or Cloud Manager)
          test -f ${workdir}/.ops-manager-env && source ${workdir}/.ops-manager-env

          # Will generate a random namespace to use during this run
          source scripts/funcs
          export PROJECT_NAMESPACE=$(generate_random_namespace)
          echo "Using $PROJECT_NAMESPACE"

          if [ "${from_version}" = "latest" ]; then
              # gets the last version released.
              export CURRENT_VERSION=$(grep mongodbOperator release.json | awk '{ print $2 }' | cut -d '"' -f 2)
          else
              export CURRENT_VERSION=${from_version}
          fi

          export CURRENT_VERSION=${from_version}
          echo "Setting up environment with current operator release: $CURRENT_VERSION"

          export REGISTRY="quay.io/mongodb"
          # By setting CURRENT_VERSION we are indicating the e2e_tests script to install this version
          # which is the latest released version, as defined in the release.json file.
          export OPERATOR_UPGRADE_IN_PROGRESS="stage1"
          TASK_NAME=${mark_build} ./scripts/evergreen/e2e_tests.sh

          # Before running the next test, the CURRENT_VERSION variable is unset, then the latest
          # Version of the operator will be installed, effectively, updating the currently running operator
          unset CURRENT_VERSION
          export OPERATOR_UPGRADE_IN_PROGRESS="stage2"

          export REGISTRY="268558157000.dkr.ecr.us-east-1.amazonaws.com/dev/${distro}"
          echo "Deploying the new Operator with version $VERSION_ID"
          TASK_NAME=${mark_verify} ./scripts/evergreen/e2e_tests.sh

  "prepare_test_env":
    - *omkp_setup
    - *configure_cloud_manager_qa
    - command: shell.exec
      params:
        working_dir: src/github.com/10gen/ops-manager-kubernetes/scripts/evergreen
        script: |
          export PATH=${workdir}/bin:$PATH
          export AWS_ACCESS_KEY_ID=${mms_eng_test_aws_access_key}
          export AWS_SECRET_ACCESS_KEY=${mms_eng_test_aws_secret}
          export AWS_REGION=${mms_eng_test_aws_region}
          export AWS_DEFAULT_REGION=${mms_eng_test_aws_region}

          test -f ${workdir}/.ops-manager-env && source ${workdir}/.ops-manager-env

          echo "Preparing Test Environment"
          ./prepare_test_env ${ops_manager_namespace} ${ops_manager_version} ${node_port}

  "post_task":
  - command: shell.exec
    params:
      script: |
        if [ ! "$(which docker)" ]; then
          # if not docker present, no images either.
          exit 0
        fi

        echo "Removing all docker images in this machine"
        docker rmi -f $(docker images -a -q) &> /dev/null || true

        echo "Are there any remaining Docker images in this host?"
        docker images


post:
  - func: "post_task"

tasks:
- name: unit_tests
  tags: ["unit_tests"]
  commands:
    - func: "test_operator"
    - func: "gotest_parse_files"

- name: unit_tests_lint
  tags: ["unit_tests"]
  commands:
    - func: "golint"

- name: release_operator
  tags: ["release_operator"]
  patch_only: true
  commands:
    - func: "clone"
    - func: "build_operator"
      vars:
        distro: ubuntu
    - func: "build_database"
      vars:
        distro: ubuntu
    - func: "setup_docker"
    - func: "build_and_push_image"
      vars:
        release_object: "mongodbOperator"
        env: production
        image_name: mongodb-enterprise-operator
    - func: "build_and_push_image"
      vars:
        release_object: "mongodbOperator"
        env: production
        image_name: mongodb-enterprise-database

- name: release_operator_rhel_connect
  tags: ["release_operator_rhel"]
  patch_only: true
  commands:
    - func: clone
    - func: "setup_jq"
    - func: build_rhel_images

- name: build_operator_rhel_connect
  tags: ["build_operator_rhel"]
  patch_only: true
  commands:
    - func: "clone"
    - func: "setup_jq"
    - func: "build_operator"
      vars:
        distro: rhel
    - func: "build_database"
      vars:
        distro: rhel
    - func: "push_operator_binary_for_rhel"
    - func: "push_database_readiness_probe_for_rhel"

- name: build_test_image
  exec_timeout_secs: 1200
  priority: 1
  commands:
    - func: clone
    - func: setup_aws
    - func: setup_jq
    - func: setup_docker
    - func: build_and_push_image
      vars:
        env: development
        image_name: mongodb-enterprise-tests
        tag_images_with_version: "true"

- name: build_images_rhel
  commands:
  - func: clone
  - func: setup_jq
  - func: setup_aws
  - func: build_operator
    vars:
      distro: rhel
  - func: build_database
    vars:
      distro: rhel
  - func: upload_e2e_build_context
    vars:
      distro: rhel
  - func: download_kubectl
  - func: setup_kubernetes_environment
    vars:
      kube_environment_name: openshift_3_11
  - func: build_e2e_images
    vars:
      distro: rhel

- name: build_images_ubuntu
  commands:
  - func: clone
  - func: setup_jq
  - func: setup_aws
  - func: build_operator
    vars:
      distro: ubuntu
  - func: build_database
    vars:
      distro: ubuntu
  - func: upload_e2e_build_context
    vars:
      distro: ubuntu
  - func: download_kubectl
  - func: setup_kubernetes_environment
    vars:
      kube_environment_name: openshift_3_11
  - func: build_e2e_images
    vars:
      distro: ubuntu

# Note that this task doesn't work so far as it needs different quay account and token
- name: release_mongodb_enterprise
  tags: ["release_mongodb_enterprise"]
  patch_only: true
  commands:
    - func: "clone"
    - func: "setup_docker"
    - func: "build_and_push_image"
      vars:
        release_object: "mongodbEnterprise"
        env: production
        image_name: mongodb-enterprise
        path: mongodb-enterprise/4.0

- name: prepare_cluster_openshift_3_11
  exec_timeout_secs: 1200
  priority: 1
  commands:
    - func: clone
    - func: setup_aws
    - func: download_kubectl
    - func: setup_kubernetes_environment
      vars:
        kube_environment_name: openshift_3_11
    - func: setup_jq
    - func: prepare_test_env
      vars:
        <<: *ops_manager_42_current

- name: prepare_cluster_vanilla_1_15
  exec_timeout_secs: 1200
  priority: 1
  commands:
    - func: clone
    - func: setup_aws
    - func: setup_jq
    - func: download_kubectl
    - func: setup_kubernetes_environment
      vars:
        kube_environment_name: vanilla_1_15
    - func: prepare_test_env
      vars:
        <<: *ops_manager_40_first

    - func: prepare_test_env
      vars:
        <<: *ops_manager_42_current

- name: prepare_cluster_vanilla_1_15_om
  exec_timeout_secs: 600
  priority: 1
  commands:
    - func: clone
    - func: setup_aws
    - func: download_kubectl
    - func: setup_kubernetes_environment
      vars:
        kube_environment_name: vanilla_1_15
        cluster_name: e2e.om.mongokubernetes.com
    - func: prepare_test_env

- name: e2e_operator_upgrade_multiple_clusters_allowed
  tags: ["openshift-om-qa"]
  exec_timeout_secs: 2400
  commands:
    - func: e2e_test_upgrade_operator
      vars:
        from_version: 1.2.2
        mark_build: e2e_operator_upgrade_build_deployment
        mark_verify: e2e_operator_upgrade_scale_and_verify_deployment

- name: e2e_operator_upgrade_from_previous
  tags: ["openshift-om-qa"]
  exec_timeout_secs: 1200
  commands:
    - func: e2e_test_upgrade_operator
      vars:
        from_version: latest
        mark_build: e2e_latest_to_current_build
        mark_verify: e2e_latest_to_current_verify

- name: e2e_multiple_cluster_failures
  tags: ["openshift-om-qa"]
  commands:
    - func: e2e_test

- name: e2e_standalone_schema_validation
  tags: ["kops-om-40"]
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_replica_set_schema_validation
  tags: ["openshift-om-qa"]
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_sharded_cluster_schema_validation
  tags: ["openshift-om-42"]
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_users_schema_validation
  tags: ["kops-om-40"]
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_standalone_config_map
  tags: ["openshift-om-qa"]
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_standalone_groups
  tags: ["openshift-om-42"]
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_standalone_recovery
  tags: ["kops-om-40"]
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_standalone_recovery_k8s
  tags: ["openshift-om-qa"]
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_replica_set
  tags: ["openshift-om-42"]
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_replica_set_recovery
  tags: ["kops-om-40"]
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_replica_set_config_map
  tags: ["openshift-om-qa"]
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_replica_set_ent
  tags: ["openshift-om-42"]
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_replica_set_groups
  tags: ["openshift-om-42"]
  exec_timeout_secs: 600
  commands:
     - func: "e2e_test"

- name: e2e_replica_set_pv
  tags: ["kops-om-40"]
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_replica_set_pv_multiple
  tags: ["openshift-om-qa"]
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_replica_set_8_members
  tags: ["openshift-om-42"]
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_replica_set_exposed_externally
  tags: ["openshift-om-qa"]
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_replica_set_readiness_probe
  tags: ["openshift-om-42"]
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_sharded_cluster
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_sharded_cluster_pv
  tags: ["kops-om-40"]
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_sharded_cluster_recovery
  tags: ["openshift-om-qa"]
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_sharded_cluster_secret
  tags: ["openshift-om-42"]
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_sharded_cluster_scale_shards
  tags: ["kops-om-42"]
  exec_timeout_secs: 720
  commands:
    - func: "e2e_test"

- name: e2e_standalone_type_change_recovery
  tags: ["openshift-om-qa"]
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_all_mongodb_resources_parallel
  tags: ["openshift-om-42"]
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_standalone_upgrade_downgrade
  tags: ["kops-om-40"]
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_replica_set_upgrade_downgrade
  tags: ["openshift-om-qa"]
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_sharded_cluster_upgrade_downgrade
  tags: ["openshift-om-42"]
  exec_timeout_secs: 1800
  commands:
    - func: "e2e_test"

- name: e2e_standalone_no_tls_no_status_is_set
  tags: ["kops-om-40"]
  exec_timeout_secs: 1200
  commands:
  - func: "e2e_test"

- name: e2e_replica_set_tls_allow
  tags: ["kops-om-42"]
  exec_timeout_secs: 1200
  commands:
  - func: "e2e_test"

- name: e2e_replica_set_tls_prefer
  tags: ["openshift-om-qa"]
  exec_timeout_secs: 1200
  commands:
  - func: "e2e_test"

- name: e2e_replica_set_tls_require
  tags: ["kops-om-42"]
  exec_timeout_secs: 1200
  commands:
  - func: "e2e_test"

- name: e2e_replica_set_tls_require_custom_ca
  tags: ["openshift-om-42"]
  exec_timeout_secs: 1200
  commands:
  - func: "e2e_test"

- name: e2e_sharded_cluster_tls_require_custom_ca
  tags: ["kops-om-42"]
  exec_timeout_secs: 1200
  commands:
  - func: "e2e_test"

- name: e2e_tls_x509_sc_custom_ca
  tags: ["openshift-om-qa"]
  exec_timeout_secs: 1200
  commands:
  - func: "e2e_test"

- name: e2e_replica_set_tls_require_and_disable
  tags: ["kops-om-42"]
  exec_timeout_secs: 1200
  commands:
  - func: "e2e_test"

- name: e2e_tls_multiple_different_ssl_configs
  tags: ["openshift-om-42"]
  exec_timeout_secs: 1200
  commands:
  - func: "e2e_test"

- name: e2e_replica_set_tls_require_upgrade
  tags: ["kops-om-42"]
  exec_timeout_secs: 1200
  commands:
  - func: "e2e_test"

- name: e2e_sharded_cluster_tls_require
  tags: ["openshift-om-qa"]
  exec_timeout_secs: 1200
  commands:
  - func: "e2e_test"

- name: e2e_tls_x509_rs
  tags: ["kops-om-42"]
  # longer timeout than usual as this test tests recovery from bad states which can take some time
  exec_timeout_secs: 1800
  commands:
  - func: "e2e_test"

- name: e2e_tls_x509_sc
  tags: ["openshift-om-42"]
  exec_timeout_secs: 1800
  commands:
    - func: "e2e_test"

- name: e2e_tls_x509_users_addition_removal
  tags: ["kops-om-42"]
  exec_timeout_secs: 1800
  commands:
    - func: "e2e_test"

- name: e2e_tls_x509_user_connectivity
  tags: ["openshift-om-qa"]
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_tls_x509_configure_all_options_rs
  tags: ["kops-om-42"]
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_tls_x509_configure_all_options_sc
  tags: ["openshift-om-42"]
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_om_appdb_scale_up_down
  tags: ["kops-ops-manager"]
  exec_timeout_secs: 2400
  commands:
    - func: "e2e_test"

- name: e2e_om_appdb_upgrade
  tags: ["kops-ops-manager"]
  exec_timeout_secs: 2400
  commands:
    - func: "e2e_test"

- name: e2e_om_ops_manager_upgrade
  tags: ["kops-ops-manager"]
  exec_timeout_secs: 2400
  commands:
    - func: "e2e_test"

- name: e2e_om_appdb_validation
  tags: ["kops-ops-manager"]
  exec_timeout_secs: 600
  commands:
    - func: "e2e_test"

- name: e2e_replica_set_scram_sha_256_user_connectivity
  tags: ["kops-om-42"]
  exec_timeout_secs: 1800
  commands:
    - func: "e2e_test"

- name: e2e_replica_set_scram_sha_1_user_connectivity
  tags: ["kops-om-42"]
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_sharded_cluster_scram_sha_256_user_connectivity
  tags: ["kops-om-42"]
  exec_timeout_secs: 1800
  commands:
    - func: "e2e_test"

- name: e2e_sharded_cluster_scram_sha_1_user_connectivity
  tags: ["kops-om-42"]
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_replica_set_scram_sha_1_upgrade
  tags: ["kops-om-42"]
  exec_timeout_secs: 1800
  commands:
    - func: "e2e_test"

- name: e2e_sharded_cluster_scram_sha_1_upgrade
  tags: ["kops-om-42"]
  exec_timeout_secs: 1800
  commands:
    - func: "e2e_test"

- name: e2e_replica_set_x509_to_scram_transition
  tags: ["kops-om-42"]
  exec_timeout_secs: 1800
  commands:
    - func: "e2e_test"

- name: e2e_sharded_cluster_x509_to_scram_transition
  tags: ["kops-om-42"]
  exec_timeout_secs: 1800
  commands:
    - func: "e2e_test"

- name: e2e_replica_set_scram_sha_and_x509
  tags: ["kops-om-42"]
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_sharded_cluster_scram_sha_and_x509
  tags: ["kops-om-42"]
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_configure_tls_and_x509_simultaneously_rs
  tags: ["kops-om-42"]
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_configure_tls_and_x509_simultaneously_sc
  tags: ["kops-om-42"]
  exec_timeout_secs: 1800
  commands:
    - func: "e2e_test"

- name: e2e_configure_tls_and_x509_simultaneously_st
  tags: ["kops-om-42"]
  exec_timeout_secs: 1200
  commands:
    - func: "e2e_test"

- name: e2e_om_ops_manager_scale
  tags: ["kops-ops-manager"]
  exec_timeout_secs: 2800
  commands:
    - func: "e2e_test"

task_groups:
- name: unit_task_group
  setup_group:
    - func: "clone"
  tasks:
    - unit_tests
    - unit_tests_lint

# This is the task group for Kubernetes-related e2e tests which focus on testing Kubernetes features instead of Automation Config
- name: e2e_kube_only_task_group
  max_hosts: 4
  setup_group:
    - func: clone
    - func: download_kubectl
    - func: setup_kubernetes_environment
  tasks:
    - e2e_replica_set_config_map
    - e2e_replica_set_exposed_externally
    - e2e_replica_set_pv
    - e2e_replica_set_pv_multiple
    - e2e_replica_set_schema_validation
    - e2e_sharded_cluster_pv
    - e2e_sharded_cluster_recovery
    - e2e_sharded_cluster_schema_validation
    - e2e_standalone_config_map
    - e2e_standalone_recovery
    - e2e_standalone_recovery_k8s
    - e2e_standalone_schema_validation
    - e2e_users_schema_validation
  teardown_task:
    - func: "upload_e2e_logs"

  # This is the general task group which is supposed to be tested on ALL OM versions. Please don't add tests here which
  # don't test anything special in automation config and more focused on Kubernetes features - such tests should go to
  # 'e2e_kube_only_task_group' task group
- name: e2e_core_task_group
  max_hosts: 4
  setup_group:
    - func: clone
    - func: download_kubectl
    - func: setup_kubernetes_environment
  tasks:
    - e2e_all_mongodb_resources_parallel
    - e2e_multiple_cluster_failures
    - e2e_operator_upgrade_from_previous
    - e2e_operator_upgrade_multiple_clusters_allowed
    - e2e_replica_set
    - e2e_replica_set_8_members
    - e2e_replica_set_ent
    - e2e_replica_set_groups
    - e2e_replica_set_recovery
    - e2e_replica_set_upgrade_downgrade
    - e2e_sharded_cluster
    - e2e_sharded_cluster_secret
    - e2e_sharded_cluster_upgrade_downgrade
    - e2e_standalone_groups
    - e2e_standalone_type_change_recovery
    - e2e_standalone_upgrade_downgrade
  teardown_task:
    - func: "upload_e2e_logs"

- name: e2e_tls_task_group
  max_hosts: 3
  setup_group:
    - func: clone
    - func: download_kubectl
    - func: setup_kubernetes_environment

  tasks:
    - e2e_replica_set_tls_allow
    - e2e_replica_set_tls_prefer
    - e2e_replica_set_tls_require
    - e2e_replica_set_tls_require_upgrade
    - e2e_replica_set_tls_require_and_disable
    - e2e_sharded_cluster_tls_require
    - e2e_standalone_no_tls_no_status_is_set
  teardown_task:
    - func: "upload_e2e_logs"

- name: e2e_scram_sha_task_group
  max_hosts: 3
  setup_group:
    - func: clone
    - func: download_kubectl
    - func: setup_kubernetes_environment

  tasks:
    - e2e_replica_set_scram_sha_256_user_connectivity
    - e2e_replica_set_scram_sha_1_user_connectivity
    - e2e_replica_set_scram_sha_1_upgrade
    - e2e_replica_set_x509_to_scram_transition
    - e2e_replica_set_scram_sha_and_x509
    - e2e_sharded_cluster_scram_sha_1_upgrade
    - e2e_sharded_cluster_x509_to_scram_transition
    - e2e_sharded_cluster_scram_sha_256_user_connectivity
    - e2e_sharded_cluster_scram_sha_1_user_connectivity
    - e2e_sharded_cluster_scram_sha_and_x509

  teardown_task:
    - func: "upload_e2e_logs"

- name: e2e_tls_custom_ca_task_group
  setup_group:
    - func: clone
    - func: download_kubectl
    - func: setup_kubernetes_environment

  tasks:
    - e2e_replica_set_tls_require_custom_ca
    - e2e_sharded_cluster_tls_require_custom_ca
    - e2e_tls_x509_sc_custom_ca
  teardown_task:
    - func: "upload_e2e_logs"

# The "others" tasks are those which cannot be run on 4.0 - only on 4.2+
- name: e2e_om_4_2_plus_only_task_group
  max_hosts: 1
  setup_group:
    - func: clone
    - func: download_kubectl
    - func: setup_kubernetes_environment
  tasks:
    - e2e_sharded_cluster_scale_shards
    - e2e_replica_set_readiness_probe
  teardown_task:
    - func: "upload_e2e_logs"

- name: e2e_x509_task_group
  max_hosts: 3
  setup_group:
    - func: clone
    - func: download_kubectl
    - func: setup_kubernetes_environment
  tasks:
    - e2e_configure_tls_and_x509_simultaneously_st
    - e2e_configure_tls_and_x509_simultaneously_rs
    - e2e_configure_tls_and_x509_simultaneously_sc
    - e2e_tls_x509_rs
    - e2e_tls_x509_sc
    - e2e_tls_x509_configure_all_options_rs
    - e2e_tls_x509_configure_all_options_sc
    - e2e_tls_x509_user_connectivity
    - e2e_tls_x509_users_addition_removal
  teardown_task:
    - func: "upload_e2e_logs"

- name: e2e_ops_manager_task_group
  max_hosts: 3
  setup_group:
    - func: clone
    - func: download_kubectl
    - func: setup_kubernetes_environment
  tasks:
    - e2e_om_appdb_scale_up_down
    - e2e_om_appdb_upgrade
    - e2e_om_appdb_validation
    - e2e_om_ops_manager_scale
    - e2e_om_ops_manager_upgrade
  teardown_task:
    - func: "upload_e2e_logs"

#
# -BUILDVARIANTS-
#
buildvariants:
- name: release_operator
  display_name: "release_operator"
  run_on:
    - archlinux-test
  tasks:
    - ".release_operator"

- name: release_operator_rhel
  display_name: "release_operator_rhel"
  run_on:
    - archlinux-test
  tasks:
    - ".release_operator_rhel"
    - ".build_operator_rhel"

- name: e2e_kube_vanilla_v1.15_om_40_first
  display_name: "e2e_kube_vanilla_v1.15_om_40_first"
  depends_on:
    - name: build_images_ubuntu
      variant: init_test_run
    - name: build_test_image
      variant: init_test_run
    - name: prepare_cluster_vanilla_1_15
      variant: init_test_run
  run_on:
  - archlinux-test
  stepback: false
  expansions:
    <<: *ops_manager_40_first
    kube_environment_name: vanilla_1_15
    distro: ubuntu
  tasks:
  - name: "e2e_core_task_group"
  - name: "e2e_tls_task_group"

# Note that both kops (current build variant) and openshift (next build variant) work with the same version of OM
# so in general they shouldn't intersect on OM-related tasks. Currently they intersect only on 'e2e_kube_only_task_group'
# and 'e2e_om_4_2_plus_only_task_group' as the latter must be run on both kops and openshift
- name: e2e_kube_vanilla_v1.15_om_42_current
  display_name: "e2e_kube_vanilla_v1.15_om_42_current"
  depends_on:
    - name: build_images_ubuntu
      variant: init_test_run
    - name: build_test_image
      variant: init_test_run
    - name: prepare_cluster_vanilla_1_15
      variant: init_test_run
  run_on:
  - archlinux-test
  stepback: false
  expansions:
    <<: *ops_manager_42_current
    kube_environment_name: vanilla_1_15
    distro: ubuntu
  tasks:
  - name: "e2e_kube_only_task_group"
  - name: "e2e_om_4_2_plus_only_task_group"
  - name: "e2e_tls_task_group"
  - name: "e2e_scram_sha_task_group"
  - name: "e2e_x509_task_group"

- name: e2e_openshift_origin_v3.11_om_42_current
  display_name: "e2e_openshift_origin_v3.11_om_42_current"
  depends_on:
    - name: build_images_rhel
      variant: init_test_run
    - name: build_test_image
      variant: init_test_run
    - name: prepare_cluster_openshift_3_11
      variant: init_test_run
  run_on:
  - archlinux-test
  stepback: false
  expansions:
    <<: *ops_manager_42_current
    kube_environment_name: openshift_3_11
    distro: rhel
  tasks:
    # Before adding the task / task_group to the current build variant check the comment on the previous one
    # ('e2e_kube_vanilla_v1.15_om_42_current')
  - name: "e2e_kube_only_task_group"
  - name: "e2e_om_4_2_plus_only_task_group"
  - name: "e2e_core_task_group"
  - name: "e2e_tls_custom_ca_task_group"

- name: e2e_openshift_cloud_qa
  display_name: "e2e_openshift_cloud_qa"
  depends_on:
    - name: build_images_rhel
      variant: init_test_run
    - name: build_test_image
      variant: init_test_run
  run_on:
  - archlinux-test
  stepback: false
  expansions:
    <<: *cloud_manager_qa
    kube_environment_name: openshift_3_11
    distro: rhel
  tasks:
  - name: "e2e_core_task_group"
  - name: "e2e_tls_task_group"
  - name: "e2e_x509_task_group"
  - name: "e2e_tls_custom_ca_task_group"
  - name: "e2e_scram_sha_task_group"

- name: e2e_kube_vanilla_ops_manager
  display_name: "e2e_kube_vanilla_ops_manager"
  depends_on:
    - name: build_images_ubuntu
      variant: init_test_run
    - name: build_test_image
      variant: init_test_run
    - name: prepare_cluster_vanilla_1_15_om
      variant: init_test_run
  run_on:
    - archlinux-test
  stepback: false
  expansions:
    cluster_name: "e2e.om.mongokubernetes.com"
    kube_environment_name: vanilla_1_15
    test_mode: "opsmanager"
    distro: ubuntu
  tasks:
    - name: "e2e_ops_manager_task_group"

# TODO: most of the OM tasks fail in Openshift
#
#- name: e2e_openshift_origin_v3.11_ops_manager
#  display_name: "e2e_openshift_origin_v3.11_ops_manager"
#  depends_on:
#    - name: build_images_rhel
#      variant: init_test_run
#    - name: prepare_cluster_openshift_3_11
#      variant: init_test_run
#  run_on:
#    - archlinux-test
#  stepback: false
#  expansions:
#    <<: *ops_manager_42_current
#    kube_environment_name: openshift_3_11
#    test_mode: "opsmanager"
#  tasks:
#    - name: "e2e_ops_manager_task_group"

- name: init_test_run
  display_name: init_test_run
  run_on:
  - archlinux-test
  stepback: false
  tasks:
  - name: build_images_rhel
  - name: build_images_ubuntu
  - name: build_test_image
  - name: prepare_cluster_openshift_3_11
  - name: prepare_cluster_vanilla_1_15
  - name: prepare_cluster_vanilla_1_15_om

#
# TODO(rodrigo): There's no mechanism that I know of to avoid running this
#                build variant at master-merge time. Disabling it for now.
#                This will be improved as part of CLOUDP-38550
#
# - name: openshift_rhel_prebuilt_images
#   display_name: openshift_rhel_prebuilt_images
#   depends_on:
#   - name: build_images_rhel
#     variant: init_test_run
#   run_on:
#     - archlinux-test
#   expansions:
#     <<: *rhel_prebuilt_images
#     <<: *cloud_manager_qa
#     kube_environment_name: openshift_3_11
#     distro: rhel
#   tasks:
#     - name: e2e_core_task_group
#     - name: e2e_tls_task_group
#     - name: e2e_x509_task_group

- name: go_unit_tests
  display_name: "go_unit_tests"
  run_on:
  - archlinux-test
  stepback: false
  tasks:
    - name: "unit_task_group"
